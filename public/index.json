[{"authors":null,"categories":null,"content":"PLAY aims to set new standards for conducting open, transparent, and reproducible behavioral science by i) publishing the protocol, and ii) making extensive use of video exemplars to demonstrate phenomena and illustrate behavioral codes. For confidentiality reasons, access to video exemplars is restricted to researchers with authorized access to Databrary. To register for access, visit http://databrary.org/register.\n","date":1546146000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1546837200,"objectID":"fda37abee07181dc9c43df33713fd613","permalink":"/study/protocol/","publishdate":"2018-12-30T00:00:00-05:00","relpermalink":"/study/protocol/","section":"study","summary":"PLAY aims to set new standards for conducting open, transparent, and reproducible behavioral science by i) publishing the protocol, and ii) making extensive use of video exemplars to demonstrate phenomena and illustrate behavioral codes. For confidentiality reasons, access to video exemplars is restricted to researchers with authorized access to Databrary. To register for access, visit http://databrary.org/register.","tags":null,"title":"Data collection protocol","type":"docs"},{"authors":null,"categories":null,"content":" PLAY involves a large-scale collaboration among laboratories across the U.S. and Canada. This page provides information about the Principal Investigators and Staff.\nPrincipal Investigators Karen E. Adolph, Ph.D. New York University Principal Investigator\nCatherine Tamis-LeMonda, Ph.D. New York University Co-Principal Investigator\nRick O. Gilmore, Ph.D. The Pennsylvania State University Co-Principal Investigator Associate Professor of Psychology rick-gilmore.com\nStaff Kasey Soska, Ph.D. Scientific Director\nOrit Hertzberg Keller Research Scientist\nSwapnaa Jayaraman, Ph.D. Research Scientist\nMelody Xu, B.S. Program Coordinator\n","date":1536465600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1546146000,"objectID":"b3025bea78946002ec5e3b2f67147aa2","permalink":"/study/people/","publishdate":"2018-09-09T00:00:00-04:00","relpermalink":"/study/people/","section":"study","summary":"PLAY involves a large-scale collaboration among laboratories across the U.S. and Canada. This page provides information about the Principal Investigators and Staff.\nPrincipal Investigators Karen E. Adolph, Ph.D. New York University Principal Investigator\nCatherine Tamis-LeMonda, Ph.D. New York University Co-Principal Investigator\nRick O. Gilmore, Ph.D. The Pennsylvania State University Co-Principal Investigator Associate Professor of Psychology rick-gilmore.com\nStaff Kasey Soska, Ph.D. Scientific Director\nOrit Hertzberg Keller Research Scientist\nSwapnaa Jayaraman, Ph.","tags":null,"title":"","type":"docs"},{"authors":null,"categories":null,"content":"All of the 1-hour observation videos will be coded using Datavyu. These pages describe the coding definitions and protocol in depth.\n","date":1536465600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536465600,"objectID":"fc13c6ec851dc7633f25bb484756db6a","permalink":"/study/coding/","publishdate":"2018-09-09T00:00:00-04:00","relpermalink":"/study/coding/","section":"study","summary":"All of the 1-hour observation videos will be coded using Datavyu. These pages describe the coding definitions and protocol in depth.","tags":null,"title":"Coding","type":"docs"},{"authors":null,"categories":null,"content":"These pages describe the PLAY study protocol in full.\n","date":1536465600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536465600,"objectID":"56c5631faa63561144645d82671f4ffd","permalink":"/study/data/","publishdate":"2018-09-09T00:00:00-04:00","relpermalink":"/study/data/","section":"study","summary":"These pages describe the PLAY study protocol in full.","tags":null,"title":"PLAY data","type":"docs"},{"authors":null,"categories":null,"content":"These pages describe the PLAY study protocol in full.\n","date":1536465600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536465600,"objectID":"e3740b2171fc4608f6c42750b19053b1","permalink":"/study/","publishdate":"2018-09-09T00:00:00-04:00","relpermalink":"/study/","section":"study","summary":"These pages describe the PLAY study protocol in full.","tags":null,"title":"Study protocol","type":"docs"},{"authors":null,"categories":null,"content":" Inclusion/Exclusion Criteria Infants\u0026rsquo; natural play in the home is characterized by tremendous variability including variations in: geographic location, climate, socioeconomic status (SES), maternal/paternal employment, childcare experiences, infants’ and mothers’ ages, language environment, physical layout and characteristics of the home, availability of media, toys for play, and so on. Researchers will be able to explore the effects of any/all such factors.\nHowever, to ensure a sufficient sample size and based on conversations with the launch group, we will limit variability along several dimensions. To be included in the PLAY sample of 900 sessions, families must be two-parent households. All infants must be English or Spanish monolingual or bilingual. All infants must be the firstborn child and 12, 18, or 24 months of age (plus/minus one week). All infants must be full-term (37-40 weeks) without known disabilities. The mother must act as the caregiver during visits, which will be scheduled at a time when only the mother and infant are present in the home.\nScheduling Visit Initial recruiting call  Your browser does not support the video tag.   https://nyu.databrary.org/slot/14765/0,79273/asset/64898/download?inline=true\nHi, may I speak with [MOM]?\nMy name is [CALLER NAME] and I’m calling from [LAB]. We have a study for [12 / 18 / 24]-month-olds and [CHILD] is the perfect age. Can I tell you about it?\nWhat language(s) do you speak to [CHILD]?\n→ If not ENGLISH or SPANISH: end call\nTo control for differences in communication, we are looking for families who speak mainly English or Spanish. Would it be alright if you are contacted for other studies in the future?\n→ If yes: continue\nFor this study, we are interested in learning about babies’ natural, everyday experiences in their homes–such as the toys they play with and places they go. For this study, a researcher will visit you and [CHILD] in your home. You and [CHILD] will be video recorded for 1 hour as you go about your day. At the end of the visit we will ask questions about your family, your home, and [CHILD]’s skills and routines. We will also ask you to take us through your home as we do a video tour capturing the places [CHILD] gets to be throughout the day and things that [he/she] plays with.\nThe study will take about 2 hours. You will receive XXX for your participation. We will schedule a day and time that’s convenient for you and when [CHILD] is usually awake/alert and not during a typical meal time.\nThe data collected in this study are valuable and will be placed in a secure web-based library available only to researchers. The purpose is to share the data with experts in the field so that scientists can learn more about infant development.\nAre you interested in participating?\n→ If yes:\nIs there a day and time that works best for you (when [CHILD] is awake/alert and not a typical meal time)?\n→ If no:\nOk thank you. May we call you for other studies?\nVoicemail Hi, this message is for [MOM]. My name is [NAME] and I’m calling from [LAB]. I’m calling because we have a fun study for [12 / 18 / 24]-month-olds and [CHILD] is the perfect age. If you are interested in hearing more about the study, please give us a call back. Our phone number is [XXX-XXX-XXXX]. Thank you and we hope to hear from you soon!\nConfirming the visit (2 days before actual visit, email the day before) 12-mo crawler  Your browser does not support the video tag.   https://nyu.databrary.org/slot/14574/-/asset/61425/download?inline=true\n12-mo walker  Your browser does not support the video tag.   https://nyu.databrary.org/slot/14167/-/asset/59866/download?inline=true\n18-mo  Your browser does not support the video tag.   https://nyu.databrary.org/slot/14513/-/asset/61070/download?inline=true\n24-mo  Your browser does not support the video tag.   https://nyu.databrary.org/slot/14514/-/asset/61427/download?inline=true\nHi, may I speak with [MOM]?\nMy name is [NAME] and I’m calling from [LAB] to confirm our visit on [DATE]. Before the visit, I’d like to ask you a few questions. It will only take 5 minutes of your time. Can we speak now?\n→ If yes:\nJust as a reminder, the data we collect from you now and during the visit, will be shared on a web-based library only available to researchers like the professor who runs this lab.\nList of questions on the Phone Questionnaire\nPlease note that presentation and format will differ in the app.\n→ If no:\nCan I call you back today or tomorrow [before the visit]?\nSchedule call.\n","date":1546146000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546837200,"objectID":"eeaa63b6a6cd90f602e25eccf3e9493f","permalink":"/study/protocol/recruiting/","publishdate":"2018-12-30T00:00:00-05:00","relpermalink":"/study/protocol/recruiting/","section":"study","summary":"Inclusion/Exclusion Criteria Infants\u0026rsquo; natural play in the home is characterized by tremendous variability including variations in: geographic location, climate, socioeconomic status (SES), maternal/paternal employment, childcare experiences, infants’ and mothers’ ages, language environment, physical layout and characteristics of the home, availability of media, toys for play, and so on. Researchers will be able to explore the effects of any/all such factors.\nHowever, to ensure a sufficient sample size and based on conversations with the launch group, we will limit variability along several dimensions.","tags":null,"title":"","type":"docs"},{"authors":null,"categories":null,"content":" Home Visit Introduction Say to Mom:\nThanks for letting us come to your home. The visit has a few parts:\nI’ll begin by video-recording you and [CHILD] as you go about your day. I will video-record you both for 1 hour. Then, I will ask [CHILD] to play with some toys both by him/herself and with you.\nAfterwards, I will ask you some general questions about your family and home, and about [CHILD]’s skills and routines.\nYou will give me a tour of your home that I will record on video to get a sense of the places [CHILD] goes and things that he/she plays with.\nDo you have any questions? Let’s start with reading and signing the consent.\nConsent to Participate and Permission to Share Ask parent to review form asking for consent to participate in the study. When finished, give parent a moment to look over form and sign it.\nAsk parent to review form asking for permission to share videos and metadata. When finished, give parent a moment to look over the form and sign it.\nHere is the Databrary Release Language. Here are videos depicting how to ask for permission to share and a sample script.\nVisit Protocol 1: 1-Hour Natural Play Video, Shoes, \u0026amp; Noise Measurement 12-mo (crawler and walker) Crawler participant view  Your browser does not support the video tag.   https://nyu.databrary.org/slot/14574/-/asset/61390/download?inline=true\nCrawler experimenter view  Your browser does not support the video tag.   https://nyu.databrary.org/slot/14574/-/asset/61397/download?inline=true\nWalker participant view  Your browser does not support the video tag.   https://nyu.databrary.org/slot/14167/-/asset/59930/download?inline=true\nWalker experimenter view  Your browser does not support the video tag.   https://nyu.databrary.org/slot/14167/-/asset/59941/download?inline=true\n18-mo Participant view  Your browser does not support the video tag.   https://nyu.databrary.org/slot/14513/-/asset/61218/download?inline=true\nExperimenter view  Your browser does not support the video tag.   https://nyu.databrary.org/slot/14513/-/asset/61220/download?inline=true\n24-mo Participant view  Your browser does not support the video tag.   https://nyu.databrary.org/slot/14514/-/asset/61054/download?inline=true\nExperimenter view  Your browser does not support the video tag.   https://nyu.databrary.org/slot/14514/-/asset/61086/download?inline=true\nInstruction to mom:\nFor the next hour, do anything you would typically do if I weren’t here. Try to ignore me as much as possible and I will stay out of the way. I will also try not to respond to you and [CHILD] so that he/she is not distracted. You can go anywhere in your home. You can play together or not. The idea is to capture what your typical day is like.\nProcedure:\nKeep camera on the child at all times. Specifically, ensure that the child’s whole body is visible on camera. If mom is in frame, capture as much of her body as possible without compromising view of the child. Record in front or to the side of the child as much as possible. Do not zoom in. Remain at as far a distance as possible (~3 to 5 m, hugging the wall) so that the child is not distracted by your presence. Try not to interact with the child or make eye contact with the child. Just watch through the view finder of the camera.\nShoes    https://nyu.databrary.org/slot/14765/0,6640/asset/65148/download?inline=true\nIf child is wearing shoes, video-record the shoes after the session; take them off child and video the bottom, side, and top views.\nProcedure:\nZoom in with camera and comment on shoe type, heel (if any), and other observations.\nDecibel Meter Open the app on your tablet and start running it just before you begin recording the free play video portion of the visit.\nProcedure:\nOpen application (the application immediately starts recording noise levels upon startup). Place device in the most central place in the home (e.g., living room)\n2: Solitary Play 12-mo crawler \u0026amp; walker 12-mo crawler participant view    https://nyu.databrary.org/slot/14574/-/asset/61352/download?inline=true\n12-mo crawler experimenter view    https://nyu.databrary.org/slot/14574/-/asset/61358/download?inline=true\n12-mo walker participant view    https://nyu.databrary.org/slot/14167/-/asset/59918/download?inline=true\n12-mo walker experimenter view    https://nyu.databrary.org/slot/14167/-/asset/59928/download?inline=true\n18-mo 18-mo participant view    https://nyu.databrary.org/slot/14513/-/asset/61064/download?inline=true\n18-mo experimenter view    https://nyu.databrary.org/slot/14513/-/asset/61078/download?inline=true\n24-mo 24-mo participant view    https://nyu.databrary.org/slot/14514/-/asset/61052/download?inline=true\n24-mo experimenter view    https://nyu.databrary.org/slot/14514/-/asset/61060/download?inline=true\nInterviewer:\nFor the next few minutes, we want to see how [CHILD] plays by him/herself. We ask you not to distract him/her or tell him/her how to play. If [CHILD] tries to get your attention or wants to play with you, you can say, “Go play. It’s perfectly fine if he/she doesn’t play with the toy. Say to child: Here [CHILD], play with this!\nCamera:\nRecord solitary toy play so that view is on baby’s body entirely and hands on object. If child moves around, follow child and keep face in frontal view. Procedure: Set yoga mat down on the floor. Un-stack cups and arrange randomly, standing upright (out of child’s view). Place child in a sitting position on yoga mat and start timing after you present the toy. Use timer on the camera (let the timer run for a bit longer than 2 min to avoid cutting the play time short. Later we code only 2 min of engagement). After 2 minutes, say: “Great job!”\n3: Dyadic (Mother-Child) Play 12-mo crawler \u0026amp; walker [VIDEO 12-mo crawler participant view    https://nyu.databrary.org/slot/14574/-/asset/61354/download?inline=true\n12-mo crawler experimenter view    https://nyu.databrary.org/slot/14574/-/asset/61358/download?inline=true\n12-mo walker participant view    https://nyu.databrary.org/slot/14167/-/asset/59920/download?inline=true\n12-mo walker experimenter view starts at 03:40    https://nyu.databrary.org/slot/14167/-/asset/59928/download?inline=true\n18-mo 18-mo participant view    https://nyu.databrary.org/slot/14513/-/asset/61066/download?inline=true\n18-mo experimenter view starts at 3:48    https://nyu.databrary.org/slot/14513/-/asset/61078/download?inline=true\n24-mo 24-mo participant view    https://nyu.databrary.org/slot/14514/-/asset/61050/download?inline=true\n24-mo experimenter view starts at 03:32    https://nyu.databrary.org/slot/14514/-/asset/61060/download?inline=true\nInstructions:\nPlease sit next to [CHILD]. I’ll give you a toy. Please play with [CHILD].\nProcedure:\nRecord so that the child and mother’s entire body and hands are captured. Use timer on camera to time engagement. After 3 minutes, say “Great job!”\n4: Questionnaires Please note that presentation and format will differ in the app.\n12-mo 12-mo crawler    https://nyu.databrary.org/slot/14514/-/asset/61050/download?inline=true\n12-mo walker    https://nyu.databrary.org/slot/14514/-/asset/61060/download?inline=true\n18-mo    https://nyu.databrary.org/slot/14513/-/asset/61076/download?inline=true\n24-mo    https://nyu.databrary.org/slot/14514/-/asset/61088/download?inline=true\nGeneral Questionnaires Instructions:\nI have some questions for you…\n[Only give introduction to the sections that need introduction (i.e., ECBQ and MB-CDI)].\nA GoogleSheet with most of the questions in a database format can be found here.\nProcedure:\nSet up camera to record the questionnaires. You\u0026rsquo;ll need to change the battery on the camera to ensure sufficient power. Sit next to the mom so she is able to read along.\n Toys \u0026amp; Pets HOME Gender Socialization Locomotor milestones ECLS-B Health Typical Day Media time \u0026amp; use  MacArthur-Bates Communicative Development Inventory (MB-CDI) MB-CDI should be administered in the primary language of the mom. Specific instructions and procedure are included in the questionnaire.\nECBQ Read instructions on questionnaire. Give mom answer sheet with rating scale.\n5: House Walkthrough \u0026amp; Room Measurements Video House Walkthrough 12-mo Crawler    https://nyu.databrary.org/slot/14574/-/asset/61356/download?inline=true\nWalker    https://nyu.databrary.org/slot/14167/-/asset/59922/download?inline=true\n18-mo    https://nyu.databrary.org/slot/14513/-/asset/61068/download?inline=true\n24-mo    https://nyu.databrary.org/slot/14514/-/asset/61048/download?inline=true\nInstructions:\nNow, we would like to see the space that [CHILD] gets to explore throughout the day. Please give me a tour of your home as I follow with a camera, and take measurements of the spaces. As we walk around, please mention the things that [CHILD] plays with in each room. Please show me where you keep his/her clothes to give us an idea of the kinds of things he/she wears.\nProcedure (Video):\nPause at the entrance of the room.\nName the room by its function (e.g., “This is where [CHILD] sleeps”). First, pan the camera SLOWLY from Left to Right. Then, pan the camera to Floor, name the different types of surfaces in the space (hardwood, plush carpet, thin rug, linoleum, tile, etc.), and then pan to the Ceiling. Hold the camera in one hand while you take measurements of the room. Do NOT turn off the camera when walking to next room. Walk SLOWLY.\nRoom Measurements with Laser Distance Measurer Measure all rooms in the house. Room = any space used by someone on a regular basis, including: bedrooms, kitchens, bathrooms, and basements. Do not measure laundry rooms. Rooms don’t have to have windows. A room has to have a clear demarcation (e.g., a wall or an entry). If the room has a short divider (e.g., when a kitchen and a living room are divided by a counter), count as one big room and measure accordingly.\nProcedure:\nTurn measure on by pressing ON/DIST button. Make sure the laser is on. Place the base of the laser flat on the wall. Avoid moldings and door castings. Measure wall to wall, lengthwise and widthwise. If a room has an odd or asymmetrical shape (i.e., any shape other than a rectangle or a square), measure the largest rectangle or square area of the room. Press ON/DIST again to take measurement. Repeat the above for length and width. Focus camera on laser measure and read measurements out loud.\n6: Body Dimensions [TBD]\n7: Visit Wrap-up Complete home measurement, housing checklist sections of the Home Questionnaire. When you arrive back at the lab, wash all toys and equipment thoroughly. Wipe down yoga mat. Rinse nesting cups in bleach-water. Do not submerge shape sorter in water (or it will stop making noise).\n8: Visit post-processing Export questionnaire data from tablet. Upload videos, questionnaires, and house decibel data to Databrary. These pages describe the PLAY study data collection protocol in full.\n","date":1546146000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546837200,"objectID":"818d911f544b785a2c3abb74aa18c3f5","permalink":"/study/protocol/visit/","publishdate":"2018-12-30T00:00:00-05:00","relpermalink":"/study/protocol/visit/","section":"study","summary":"Home Visit Introduction Say to Mom:\nThanks for letting us come to your home. The visit has a few parts:\nI’ll begin by video-recording you and [CHILD] as you go about your day. I will video-record you both for 1 hour. Then, I will ask [CHILD] to play with some toys both by him/herself and with you.\nAfterwards, I will ask you some general questions about your family and home, and about [CHILD]’s skills and routines.","tags":null,"title":"","type":"docs"},{"authors":null,"categories":null,"content":" Preparing for Visit Prepare paperwork Write Participant ID on all paperwork (consents and questionnaires). Fill out locomotor milestone worksheet.\nPack equipment  Camera, SD card and extra battery\n Mic Laser Measure Solitary play toy Dyadic play toy Yoga mat Tablet with app for questionnaires (if mom speaks English and/or Spanish, bring both versions of MacArthur), study consent form, Databrary sharing release form, and decibel meter. Answer choice sheet with response scales Participant payment Paper copies of all questionnaires, MCDI, and consent and Databrary forms in case of tablet failure Tools for body dimensions (Height and Weight)- TBD  ","date":1546146000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546837200,"objectID":"4f23b23db6529fe2ae148d2091b95276","permalink":"/study/protocol/visit_prep/","publishdate":"2018-12-30T00:00:00-05:00","relpermalink":"/study/protocol/visit_prep/","section":"study","summary":" Preparing for Visit Prepare paperwork Write Participant ID on all paperwork (consents and questionnaires). Fill out locomotor milestone worksheet.\nPack equipment  Camera, SD card and extra battery\n Mic Laser Measure Solitary play toy Dyadic play toy Yoga mat Tablet with app for questionnaires (if mom speaks English and/or Spanish, bring both versions of MacArthur), study consent form, Databrary sharing release form, and decibel meter. Answer choice sheet with response scales Participant payment Paper copies of all questionnaires, MCDI, and consent and Databrary forms in case of tablet failure Tools for body dimensions (Height and Weight)- TBD  ","tags":null,"title":"","type":"docs"},{"authors":["Rick O. Gilmore"],"categories":null,"content":"Hi, all.\nWe have a project website and blog using the blogdown package for R, and the Hugo Academic theme. Let us know what you think.\n","date":1545932220,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545932220,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"/post/getting-started/","publishdate":"2018-12-27T12:37:00-05:00","relpermalink":"/post/getting-started/","section":"post","summary":"We have a new blog using blogdown, Hugo, and the Academic theme.","tags":["blogging"],"title":"Hello, Hugo","type":"post"},{"authors":null,"categories":null,"content":" Coding Set Up This section describes how to transcribe \u0026amp; code the 1-hour natural play session.\nGetting Started Download the development version of Datavyu. Download the PLAY_CodingTemplate.opf file from the PLAY Databrary Volume. Name this file with the PLAY naming convention (e.g., PLAY_NYU001, … PLAY_NYU010, … PLAY_NYU030). - This template contains all of the primary variables that will be coded by each site: communication, gesture, object interaction, locomotion, and emotion. Download Ruby scripts for each coding variable as needed from the PLAY Github repository.\nGet to Know Datavyu Familiarize yourself with Datavyu before you begin coding (resources on Datavyu.org, videos from past workshops, etc.). Refer to the Datavyu User Guide. Take a look at our Best Practices for Coding Behavioral Data From Video on the Datavyu site.\nCoding in Passes The coding manual describes the transcription  process and codes for 5 content areas: communication, gesture, object interaction, locomotion, and emotion. Each content area includes two passes: one pass for the infant and one pass for the mother. For gesture, the baby and mom are coded together in a single pass. A pass entails scoring the relevant codes for 1-hour of video.\nPlease visit our GitHub Repository \u0026lt;https://github.com/databrary/PLAY-Project-Datavyu-scripts\u0026gt; for all of the scripts mentioned in this wiki.  Workflow for Coding Communication Passes After the file has been transcribed according to procedure in Transcription, run two additional scripts that will prepare new Communication columns for further coding. Run splitmombaby_transcribe.rb. This script pulls out mom and baby language from the transcribe column into two new columns: (1) momspeech and (2) babyvoc. Each column is automatically populated with cells from the respectively tagged utterances from the transcribe column (e.g., the script ports all utterances coded as ‘m’ to the momspeech column and \u0026lsquo;b\u0026rsquo; to the babyvoc column). Each new cell in momspeech and babyvoc is a point cell created at the onset of each cell from the transcription. Run create_mombaby_utterancetype.rb. This script also creates two new columns: (1) momutterancetype and (2) babyutterancetype. For each cell in momspeech and babyvoc, a new cell is created in momutterancetype and babyutterancetype, respectively. The codes for these cells are blank, and the coder now scores mom and baby communication according to definitions in Communication Codes.\nWorkflow for Coding Gesture Pass Score baby and mom gesture together in a single pass according to definitions in Gesture Codes. After the gesture coding pass (for both mom and baby) has been done, run a script that will separate mom and baby gestures into two columns. Run Split-MomBabyGesture.rb. This script pulls out mom and baby gestures from the gesture column into two new columns: (1) babygesture and (2) momgesture. Each column is automatically populated with cells from the respectively tagged events from the gesture column (e.g., the script ports all gestures coded as ‘m’ to the momgesture column and \u0026lsquo;b\u0026rsquo; to the babygesture column). Each new cell in babygesture and momgesture is a point cell created at the onset of each cell in the gesture column.\nWorkflow for Object, Locomotion, and Emotion Passes Choose whether to code baby or mom first within each pass for object, locomotion, or emotion. Score each pass according to definitions in Object Codes, Locomotion Codes, or Emotion Codes. Workflow for Inter-Observer Reliability on Communication, Gesture, Object, Locomotion, and Emotion Passes After the primary coder finishes a pass: babyutterancetype, momutterancetype, gesture (split into babygesture, momgesture), babyobject, momobject, babyloc, momloc, babyemotion, or momemotion run two scripts to set up the Datavyu spreadsheet for coding reliability. First, run a script called insert-RelBlocks.rb. This script randomly generates 3, 5-minute chunks within the first, second, and third 20-minute sections of the 1-hour video of free play. By quasi-randomly inserting reliability blocks from areas of the primary coder’s pass, this will ensure that the reliability coder sees each portion of the video for each child’s session. Thus, the idiosyncrasies of each child, fluctuations over the 1-hour session, and drift in the coder are spread over the session. Reliability on each coding pass is done on the same 3, 5-minute blocks for each pass. The scripting window in Datavyu will prompt when everything has been successfully completed. You should now have a brand new column in your spreadsheet named reliability_blocks. This script should only be run once so that reliability coding can be done within the same time frame for each coding domain for each session. Now, run another script appropriate for the pass reliability needs to be coded on: CreateReliability-BabyUtterancetype.rb or CreateReliability-MomUtterancetype.rb or CreateReliability-Gesture.rb or CreateReliability-MomBaby-Loc.rb or CreateReliability-MomBaby-Object.rb OR CreateReliability-MomBaby-Emotion.rb This script inserts new coding columns where your reliability coder will score the video while they are locked into the script-generated, 5-minute chunks in the reliability_blocks column.\nCoding ID Datavyu ID Code for 1-Hour Natural Play\nMake sure you are currently logged in at Databrary to view embedded video examples in this wiki.  id \u0026lt;site\u0026gt; \u0026lt;participant\u0026gt; \u0026lt;testdate\u0026gt; \u0026lt;birthdate\u0026gt; \u0026lt;agegroup\u0026gt; \u0026lt;sex\u0026gt; \u0026lt;study\u0026gt; \u0026lt;babylanguage1\u0026gt; \u0026lt;babylanguage2\u0026gt; \u0026lt;momlanguage2\u0026gt; \u0026lt;momlanguage2\u0026gt;\nOperational Definitions \u0026lt;onset/offset\u0026gt;: Set every ID cell onset to 00:00:00:000 (hours : minutes : seconds : milliseconds).\nSet ID cell offset to the last frame in the 1-hour free play session.\n\u0026lt;site\u0026gt;: Site refers to the data collection site: New York University, Rutgers Newark, CUNY Staten Island, Penn State, etc.\nGet the site from the metadata information collected on the app.\nFormat is three letters all caps: NYU, RTG, CSI, PSU.\n\u0026lt;participant\u0026gt;: Participant number refers to the infant\u0026rsquo;s participant number in the order that the data were collected.\nParticipant numbers run consecutively from 001 within each site.\nGet the participant number from the metadata information collected on the app.\nFormat for id number is three digits 001, 012, 021.\n\u0026lt;testdate\u0026gt;: Test date is the day of the home visit.\nGet the test date from the metadata information collected on the app.\nFormat for test date is YYYY-MM-DD.\n\u0026lt;birthdate\u0026gt;: Birth date is the day the baby was born.\nGet the birth date from the metadata information collected on the app.\nFormat for birth date is YYYY-MM-DD.\n\u0026lt;agegroup\u0026gt;: Entered as 12, 18, or 24.\nGet the age group from the metadata information collected on the app.\n\u0026lt;sex\u0026gt;: Refers to infant\u0026rsquo;s biological sex.\nCode m = male/boy; f = female/girl.\nGet the sex from the metadata information collected on the app.\n\u0026lt;study\u0026gt;: Study name.\nCode as \u0026lsquo;PLAY\u0026rsquo;.\n\u0026lt;babylanguage1\u0026gt;: Refers to infant\u0026rsquo;s predominant language spoken during the session.\nCode with lowercase, full name of the language: \u0026lsquo;english\u0026rsquo; or \u0026lsquo;spanish\u0026rsquo;.\n\u0026lt;babylanguage2\u0026gt;: Refers to infant\u0026rsquo;s other language spoken during the session, if another language was spoken.\nCode with lowercase, full name of the language: \u0026lsquo;english\u0026rsquo; or \u0026lsquo;spanish\u0026rsquo;. If no other language was spoken as missing \u0026lsquo;.\u0026rsquo;\n\u0026lt;momlanguage1\u0026gt;: Refers to mother\u0026rsquo;s predominant language spoken during the session.\nCode with lowercase, full name of the language: \u0026lsquo;english\u0026rsquo; or \u0026lsquo;spanish\u0026rsquo;.\n\u0026lt;momlanguage2\u0026gt;: Refers to mother\u0026rsquo;s other language spoken during the session, if another language was spoken.\nCode with lowercase, full name of the language: \u0026lsquo;english\u0026rsquo; or \u0026lsquo;spanish\u0026rsquo;. If no other language was spoken as missing \u0026lsquo;.\u0026rsquo;\n","date":1536465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546146000,"objectID":"718a492bd89de379dd4060b6fb5fb453","permalink":"/study/coding/coding_setup/","publishdate":"2018-09-09T00:00:00-04:00","relpermalink":"/study/coding/coding_setup/","section":"study","summary":"Coding Set Up This section describes how to transcribe \u0026amp; code the 1-hour natural play session.\nGetting Started Download the development version of Datavyu. Download the PLAY_CodingTemplate.opf file from the PLAY Databrary Volume. Name this file with the PLAY naming convention (e.g., PLAY_NYU001, … PLAY_NYU010, … PLAY_NYU030). - This template contains all of the primary variables that will be coded by each site: communication, gesture, object interaction, locomotion, and emotion.","tags":null,"title":"","type":"docs"},{"authors":null,"categories":null,"content":" Coding communication babyvoc \u0026lt;content\u0026gt;\nGeneral Orientation Contains a transcript of all of the utterances/vocalizations of the baby.\nThis column is automatically populated after the transcribe pass is completed using a Ruby script. All of the utterances tagged with \u0026lsquo;b\u0026rsquo; in  in transcribe are transferred here. The onset and offset are equal, and set to the onset from the transcribe column, which reflects a time as close as possible to the onset of that utterance.\nbabyutterancetype \u0026lt;language_s-w\u0026gt; \u0026lt;langlike-b-v\u0026gt; \u0026lt;crygrunt_c-g\u0026gt; \u0026lt;unintell-x\u0026gt;\nGeneral Orientation Utterance type = categorization of previously coded utterances as a specific type of speech form. Read the utterance transcribed in babyvoc column and categorize each utterance based on codes below.\nCodes are mutually exclusive. The prompts/arguments in the code are designed to speed the coder through the easiest to detect and easiest to code categories (language, language-like sounds, etc.) down through the more nuanced and time-consuming codes. Once the proper code has been found, enter it into the prompt you are at, then code all of the rest as periods ”.”. For instance, if the baby didn\u0026rsquo;t speak in full speech, or speech-like sound, but did cry/scream, then code \u0026lt;.,.,c,.\u0026gt;.\nThe transcript will expedite this process. Double check and listen again as you read the transcript. Comment any disagreements.\nValue List \u0026lt;language_s-w\u0026gt;\ns = sentence\nw = word\n\u0026lt;langlike_b-v\u0026gt;\nb = babble\nv = vowel\n\u0026lt;crygrunt_c-g\u0026gt;\nc = cry\ng = grunt\n\u0026lt;unintell-x\u0026gt;\nx = unintelligible\nOperational Definitions \u0026lt;s\u0026gt;\nSentence = an utterance in which the speaker utters more than one word, producing a sentence or phrase (e.g., “Daddy\u0026rsquo;s shoe” or “Go to the park”).\nTranscription: “ooh gimme that”  Your browser does not support the video tag.   https://nyu.databrary.org/slot/14765/-/asset/63348/download?inline=true\nTranscription: “i take this”  Your browser does not support the video tag.   https://nyu.databrary.org/slot/14765/-/asset/63350/download?inline=true\nTranscription: “goodbye sad face?”  Your browser does not support the video tag.   https://nyu.databrary.org/slot/14765/-/asset/63354/download?inline=true\n\u0026lt;w\u0026gt;\nWord = an utterance in which the speaker utters a single word, such as “dolly” or “ball.”\nTranscription: “cars”  Your browser does not support the video tag.   https://nyu.databrary.org/slot/14765/-/asset/63352/download?inline=true\nTranscription: “basketball”  Your browser does not support the video tag.   https://nyu.databrary.org/slot/14765/-/asset/63356/download?inline=true\nTranscription: “truck”  Your browser does not support the video tag.   https://nyu.databrary.org/slot/14765/-/asset/63358/download?inline=true\n\u0026lt;b\u0026gt;\nBabble = an utterance in which the speaker utters a series of repeated canonical syllables, such as ba-ba-ba, or ga-ga-ga.\nTranscription: “b”  Your browser does not support the video tag.   https://nyu.databrary.org/slot/14765/-/asset/63334/download?inline=true\nTranscription: “b”  Your browser does not support the video tag.   https://nyu.databrary.org/slot/14765/-/asset/63338/download?inline=true\nTranscription: “b”  Your browser does not support the video tag.   https://nyu.databrary.org/slot/14765/-/asset/63340/download?inline=true\n\u0026lt;v\u0026gt;\nVowel = an utterance in which the speaker utters a vowel sound (e.g, /a/, /i:/).\nTranscription: “v”  Your browser does not support the video tag.   https://nyu.databrary.org/slot/14765/-/asset/63342/download?inline=true\nTranscription: “v”  Your browser does not support the video tag.   https://nyu.databrary.org/slot/14765/-/asset/63344/download?inline=true\nTranscription: “v”  Your browser does not support the video tag.   https://nyu.databrary.org/slot/14765/-/asset/63346/download?inline=true\n\u0026lt;c\u0026gt;\nCry = an utterance in which the speaker is experiencing a period of prolonged distress.\nTranscription: “c”  Your browser does not support the video tag.   https://nyu.databrary.org/slot/14765/-/asset/63746/download?inline=true\nTranscription: “c”  Your browser does not support the video tag.   https://nyu.databrary.org/slot/14765/-/asset/63749/download?inline=true\nTranscription: “c”  Your browser does not support the video tag.   https://nyu.databrary.org/slot/14765/-/asset/63752/download?inline=true\n\u0026lt;g\u0026gt;\nGrunt = an utterance in which the speaker produces a low, short, inarticulate, guttural sound often used to express effort or exertion. Vegetative sounds, such as coughing and sneezing, should be captured using this code.\nTranscription: “c”  Your browser does not support the video tag.   https://nyu.databrary.org/slot/14765/-/asset/63747/download?inline=true\nTranscription: “c”  Your browser does not support the video tag.   https://nyu.databrary.org/slot/14765/-/asset/63753/download?inline=true\n\u0026lt;x\u0026gt;\nUnintelligible = either what the baby said was not intelligible to the transcriber, or after listening you are not able to understand well enough what they say even with the transcript to properly code it.\nHow to Code Set the “JUMP-BACK-BY” to 2 s.\nHit “FIND” on the controller to go to the onset of each utterance, which was populated in babyvoc column. JUMP-BACK-BY 2 s so the utterance can be viewed in context.\nPlay in real time to code each utterance, which is coded in mutually exclusive categories. TAB to between each argument/prompt inserting period “.” until you reach the appropriate code. Then insert periods to the end of the cell.\nmomspeech \u0026lt;content\u0026gt;\nGeneral Orientation Contains a transcript of all of the utterances of the mom.\nThis column is automatically populated after the transcribe pass is completed using a Ruby script. All of the utterances tagged with \u0026rsquo;m\u0026rsquo; in  in transcribe are transferred here. The onset and offset are equal, and set to the onset from the transcribe column, which reflects a time as close as possible to the onset of that utterance.\nmomutterancetype \u0026lt;imperative_l-a-p\u0026gt; \u0026lt;interrog-i_declar-d\u0026gt; \u0026lt;filler-f\u0026gt; \u0026lt;unintell-x\u0026gt;\nGeneral Orientation Utterance type = categorization of previously coded utterances as a specific type of speech form. Read the utterance transcribed in momspeech column and categorize each utterance based on codes below.\nCodes are mutually exclusive. The prompts/arguments in the code are designed to speed the coder through the easiest to detect and easiest to code categories (imperatives, then interrogatives, declaratives, etc.) down through the more nuanced and time-consuming codes. After the proper code has been found, enter it into the prompt you are at, then code all of the rest as periods “.”. For instance, if the mom didn\u0026rsquo;t do an imperative, sing/read, but did give a declarative, then code \u0026lt;.,d,.,.\u0026gt;.\nWhat is coded is not solely based on the transcript. Listen to the audio, watch the video, and read the transcript so you are sure of the intent behind the mom\u0026rsquo;s speech.\nValue List \u0026lt;imperative_l-a-p\u0026gt;\nl = imperative look\na = imperative act\np = imperative prohibit\n\u0026lt;interrog-i_declar-d\u0026gt;\ni = interrogative`\nd = declarative\n\u0026lt;filler-f\u0026gt;\nf = filler/affirmation\n\u0026lt;unintell-x\u0026gt;\nx = unintelligible\nOperational Definitions \u0026lt;l\u0026gt;\nImperative Look = an utterance in which the speaker directs a baby\u0026rsquo;s attention (e.g., “Look here”, “See?”, or calls baby\u0026rsquo;s name to alert attention).\nTranscript: “evelyn”  Your browser does not support the video tag.   https://nyu.databrary.org/slot/14765/-/asset/63370/download?inline=true\nTranscript: “look”  Your browser does not support the video tag.   https://nyu.databrary.org/slot/14765/-/asset/63374/download?inline=true\n\u0026lt;a\u0026gt;\nImperative Act = an utterance in which the speaker directs a baby\u0026rsquo;s action, such as asking baby to do something, or to play with an object. An example would be if a mother tells her baby “let\u0026rsquo;s play with the ball”.\nTranscript: “turn the page”  Your browser does not support the video tag.   https://nyu.databrary.org/slot/14765/-/asset/63376/download?inline=true\nTranscript: “come here please”  Your browser does not support the video tag.   https://nyu.databrary.org/slot/14765/-/asset/63378/download?inline=true\nTranscript: “go get the basketball”  Your browser does not support the video tag.   https://nyu.databrary.org/slot/14765/-/asset/63392/download?inline=true\nNOTE: The imperative look and imperative act can be collapsed if the breakdown takes too long to code/specify (although we don\u0026rsquo;t think it will save time).\n\u0026lt;p\u0026gt;\nImperative Prohibit = an utterance in which the speaker prohibits a baby\u0026rsquo;s behavior, such as asking baby to stop what they\u0026rsquo;re doing.\nTranscript: “dont knock it over”  Your browser does not support the video tag.   https://nyu.databrary.org/slot/14765/-/asset/63380/download?inline=true\nTranscript: “dont be so rough”  Your browser does not support the video tag.   https://nyu.databrary.org/slot/14765/-/asset/63394/download?inline=true\nTranscript: “no no tv”  Your browser does not support the video tag.   https://nyu.databrary.org/slot/14765/-/asset/63755/download?inline=true\n\u0026lt;i\u0026gt;\nInterrogative = an utterance in which the speaker asks for information about objects or ongoing activities (e.g., “What is this called?”, “What color is this?). Questions that start with “Can you” or “Would you” (e.g., “Can you put that down”) should not be considered for interrogatives. Their function is to regulate the baby\u0026rsquo;s behavior and should be coded as imperatives. Tag questions, in which the speaker adds a question at the end of a statement (“That\u0026rsquo;s a blue truck, right?”) are not considered questions. These should be coded as declaratives.\nTranscript: “how does the pig say?”  Your browser does not support the video tag.   https://nyu.databrary.org/slot/14765/-/asset/63386/download?inline=true\nTranscript: “what is this?”  Your browser does not support the video tag.   https://nyu.databrary.org/slot/14765/-/asset/63390/download?inline=true\n\u0026lt;d\u0026gt;\nDeclarative= an utterance in which the speaker provides information about objects, events or ongoing activities (e.g., “This is a fun toy”; “Red truck”; “You are stirring in the cup”.\nTranscript: “baby\u0026rsquo;s clothes”  Your browser does not support the video tag.   https://nyu.databrary.org/slot/14765/-/asset/63368/download?inline=true\nTranscript: “thats a lemonade”  Your browser does not support the video tag.   https://nyu.databrary.org/slot/14765/-/asset/63388/download?inline=true\n\u0026lt;f\u0026gt;\nAffirmations/Fillers = an utterance in which the speaker is recognizing another speaker\u0026rsquo;s behavior and agreeing with it, or using words as conversational fillers. For instance, when the mother says “There you go” when the baby successfully completes a puzzle, or when she says “yeah”, or “uhuh”.\nTranscript: “wow”  Your browser does not support the video tag.   https://nyu.databrary.org/slot/14765/-/asset/63396/download?inline=true\nTranscript: “there you go”  Your browser does not support the video tag.   https://nyu.databrary.org/slot/14765/-/asset/63757/download?inline=true\n\u0026lt;x\u0026gt; Unintelligible = either what the mom said was not intelligible to the transcriber, or after listening you are not able to understand well enough what they say even with the transcript to properly code it.\nTranscript: “xxx”  Your browser does not support the video tag.   https://nyu.databrary.org/slot/14765/-/asset/63410/download?inline=true\nTranscript: “xxx”  Your browser does not support the video tag.   https://nyu.databrary.org/slot/14765/-/asset/63412/download?inline=true\nHow to Code Set the JUMP-BACK-BY key for 2 s. Hit “FIND” on the controller to go to the onset of each utterance, which was populated in momspeech column.\nJUMP-BACK-BY 2 s so the utterance can be viewed in context. Play in real time to code each utterance, which is coded in mutually exclusive categories.\nTAB between each argument/prompt inserting period ”.“ until you reach the appropriate code. Then insert periods to the end of the cell.\n","date":1536465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546146000,"objectID":"d7a68c8ebeeef1db0e86b0c780b058d5","permalink":"/study/coding/communication/","publishdate":"2018-09-09T00:00:00-04:00","relpermalink":"/study/coding/communication/","section":"study","summary":"Coding communication babyvoc \u0026lt;content\u0026gt;\nGeneral Orientation Contains a transcript of all of the utterances/vocalizations of the baby.\nThis column is automatically populated after the transcribe pass is completed using a Ruby script. All of the utterances tagged with \u0026lsquo;b\u0026rsquo; in  in transcribe are transferred here. The onset and offset are equal, and set to the onset from the transcribe column, which reflects a time as close as possible to the onset of that utterance.","tags":null,"title":"","type":"docs"},{"authors":null,"categories":null,"content":" Coding emotion babyemotion \u0026lt;emotion_p-n\u0026gt;\nGeneral Orientation This code captures the times that the baby is clearly displaying positive or negative emotion through facial expressions. Times when the baby is in neutral emotion are not marked. Bouts of emotion as scored as events, where the grey spaces between emotion events mean the baby is neutral or emotion is unclear. Coders also mark times as “missing” when the baby\u0026rsquo;s face could not possibly be coded for emotion (e.g., face completely turned away from camera, baby\u0026rsquo;s head out of the video). When the baby\u0026rsquo;s face is clearly not visible, negative emotion may be coded only if there is absolute clear vocal affect (e.g., baby is screaming and crying).\nCoders are watching/tagging the duration of each positive or negative emotion event by marking onset/offset times. To determine emotion, coders are watching the baby\u0026rsquo;s face, not vocal affect. To determine if emotion is not codeable/missing, coders are watching for when the face fully moves out of the camera view.\nValue List p = positive emotion\nn = negative emotion\n. = baby\u0026rsquo;s face is completely not visible\nOperational Definitions \u0026lt;p\u0026gt;\nCode \u0026lsquo;p\u0026rsquo; when the baby is clearly displaying positive emotion (e.g., smiling). Code based off of the face and not off of the voice. Look for raising of the corners of the mouth, grinning and showing the teeth, along with closing of the eyes because of the raised cheeks. If there is any doubt that the baby is showing positive emotion, then do not begin the code.\nPositive emotion cannot be coded based on the voice alone. So positive emotion could not be scored when the baby\u0026rsquo;s face is absolutely not visible (i.e. missing).\n\u0026lt;n\u0026gt;\nCode \u0026lsquo;n\u0026rsquo; when the baby is clearly displaying negative emotion (e.g., frowning, wincing). Code based off of the face and not off of the voice. Look for lowering of the corners of the mouth, stretching and tautness of the lips, along with closing of the eyes because of furrowed brow. If there is any doubt that the baby is showing negative emotion, then do not begin the code.\nDo not defer to the voice to code negative emotion when the face is visible. If the baby\u0026rsquo;s face is clearly not visible (i.e. missing), then \u0026lsquo;n\u0026rsquo; can be coded if the baby is displaying clear negative emotion through their voice. The baby could be screaming, crying, or yelling. If there is any doubt whether the voice is negative, then do not begin the code.\nOnset Onset of an emotion event is the first frame the baby is clearly displaying positive/negative emotion through the face. The onset does not need to be completely frame accurate, since emotion could begin in any part of the face. The coder is looking to identify when any lay person would absolutely agree the baby is showing positive/negative emotion based on the face.\nThere is no criterion for how long an emotion event should be. It is easy for the coder to mark the first frame when they see clear positive/negative emotion, even if it ends a few frames later. Events that are later deemed “too brief” could be removed via scripting.\nWhen coding \u0026lsquo;n\u0026rsquo; from voice during missing time, set the onset when the negative voice starts and end the \u0026lsquo;n\u0026rsquo; code when the voice ends. The onset/offset do not need to be frame accurate. For cases when emotion code begins right out of missing: The face has not been visible and the first frame you can see the face again the infant is clearly displaying positive/negative emotion. Code the onset of the emotion code as the first frame when the face reappears. Use the “0” key to set the onset of the emotion event and simultaneously set the offset of \u0026lsquo;missing\u0026rsquo;. We want to preserve the 1ms difference between \u0026lsquo;missing\u0026rsquo; and the emotion code so we can know that that \u0026lsquo;missing\u0026rsquo; event was ended because of the onset of an emotion code.\nOffset Offset of a positive/negative emotion is the first frame the baby is clearly back to a neutral emotion through the face. The offset does not need to completely frame accurate, since emotion could end in any part of the face. The coder is looking to identify when any lay person would absolutely agree the baby is no longer showing any positive/negative emotion based on the face.\nIf the baby\u0026rsquo;s face returns to neutral for less than 5 frames during one emotion code (e.g. positive, then neutral for 4 frames, then back to positive), continue the \u0026lsquo;p\u0026rsquo; or \u0026lsquo;n\u0026rsquo; code. The coder would have to expend unneeded effort to identify and tag those offsets and onsets time, since reliability does not need to be frame accurate.\nFor cases when emotion code is ended by missing: The emotion event may or may not have ended but the coder can no longer see the face to code offset. Code the offset as the first frame the face completely is not visible (see \u0026lsquo;missing\u0026rsquo; code below). Use “0” key to set the offset of emotion and simultaneously code the onset of \u0026lsquo;missing\u0026rsquo;. We want to preserve the 1ms difference between the emotion code and \u0026lsquo;missing\u0026rsquo; so we can know that that \u0026lsquo;missing\u0026rsquo; event caused the offset of that emotion event.\n\u0026lt;.\u0026gt;\nCode \u0026lsquo;.\u0026rsquo; for missing when the baby\u0026rsquo;s face is completely not visible. The baby\u0026rsquo;s full face could turned away from the camera, baby\u0026rsquo;s head is completely off camera, or the baby is out of frame entirely. If the coder could see the emotion the baby is expressing in their face from a side or oblique angle, then do not code missing.\nIf the face is not visible but the baby is displaying clear negative emotion in the voice (e.g., baby crying or screaming) then the missing code is ended and \u0026lsquo;n\u0026rsquo; is coded (see \u0026lsquo;n\u0026rsquo; code). Positive emotion cannot be coded by voice.\nOnset is the first frame in which the coder clearly cannot see the face. Offset is the first frame in which the coder can clearly see the face again. The onset and offset do not need to be completely frame accurate, since reliability does not need to be exact from accurate.\nIf the baby\u0026rsquo;s face was \u0026lsquo;missing\u0026rsquo; and then reappeared for less than 5 frames, don’t stop the \u0026lsquo;.\u0026rsquo; code to mark those few frames.\nHow to Code Set “JUMP-BACK-BY” key to 1 s.\nPlay with #8-PLAY in real time (1x speed) until the baby changes to clear positive or clear negative emotion or the face is clearly not visible. Focus on the baby\u0026rsquo;s face and do not be distracted by what the baby is saying or doing.\nPause with #5-STOP once you have identified a clear change in emotion or that the baby\u0026rsquo;s face is no longer visible at all. Shuttle back with #4-SHUTTLEBACK at 1\u0026frasl;8-1/4x speed to identify the onset. Use the mouth and eyes as the guide to onset. Press ENTER to set the onset as the frame where any lay person would say that baby is happy or sad. The coder may even feel happy or sad watching the baby\u0026rsquo;s face; use this as a guide for onset.\nThen hit #8-PLAY then #4-SHUTTLEBACK once to watch at 1/2x and look for the offset of that emotion or when the face comes completely back into view. For missing, if it seems like there may be a long stretch of missing (e.g. baby has completely wandered out of the room) then watch at 1x or 2x speed—but be listening in case there is negative emotion in the voice. Pause when you identify the offset.\nHit #1-JOGBACK and #3-JOGFORWARD to tag the frame the baby\u0026rsquo;s face is clearly not positive or not negative (returned to neutral) or the face is visible again to code emotion from.\nThen return to real time (1x speed) with #8-PLAY to watch for the next event.\nmomemotion \u0026lt;emotion_p-n\u0026gt;\nGeneral Orientation This code captures the times that the mom is clearly displaying positive or negative emotion through facial expressions. Times when the mom is in neutral emotion are not marked. Bouts of emotion as scored as events, where the grey spaces between emotion events mean the mom is neutral or emotion is unclear. Coders also mark times as “missing” when the mom\u0026rsquo;s face could not possibly be coded for emotion (e.g., face completely turned away from camera, mom\u0026rsquo;s head out of the video). When the mom\u0026rsquo;s face is clearly not visible, negative emotion may be coded only if there is absolute clear vocal affect (e.g., mom is yelling).\nCoders are watching/tagging the duration of each positive or negative emotion event by marking onset/offset times. To determine emotion, coders are watching the mom\u0026rsquo;s face, not vocal affect. To determine if emotion is not codeable/missing, coders are watching for when the face fully moves out of the camera view.\nValue List p = positive emotion\nn = negative emotion\n. = mom\u0026rsquo;s face is completely not visible\nOperational Definitions \u0026lt;p\u0026gt;\nCode \u0026lsquo;p\u0026rsquo; when the mom is clearly displaying positive emotion (e.g., smiling). Code based off of the face and not off of the voice. Look for raising of the corners of the mouth, grinning and showing the teeth, along with closing of the eyes because of the raised cheeks. If there is any doubt that the mom is showing positive emotion, then do not begin the code.\nPositive emotion cannot be coded based on the voice alone. So positive emotion could not be scored when the mom\u0026rsquo;s face is absolutely not visible (i.e. missing).\n\u0026lt;n\u0026gt;\nCode \u0026lsquo;n\u0026rsquo; when the mom is clearly displaying negative emotion (e.g., frowning, wincing). Code based off of the face and not off of the voice. Look for lowering of the corners of the mouth, stretching and tautness of the lips, along with closing of the eyes because of furrowed brow. If there is any doubt that the mom is showing negative emotion, then do not begin the code.\nDo not defer to the voice to code negative emotion when the face is visible. If the mom\u0026rsquo;s face is clearly not visible (i.e. missing), then \u0026lsquo;n\u0026rsquo; can be coded if the mom is displaying clear negative emotion through their voice. The mom could be screaming or upset. If there is any doubt whether the voice is negative, then do not begin the code.\nOnset Onset of an emotion event is the first frame the mom is clearly displaying positive/negative emotion through the face. The onset does not need to be completely frame accurate, since emotion could begin in any part of the face. The coder is looking to identify when any lay person would absolutely agree the mom is showing positive/negative emotion based on the face.\nThere is no criterion for how long an emotion event should be. It is easy for the coder to mark the first frame when they see clear positive/negative emotion, even if it ends a few frames later. Events that are later deemed “too brief” could be removed via scripting.\nWhen coding \u0026lsquo;n\u0026rsquo; from voice during missing time, set the onset when the negative voice starts and end the \u0026lsquo;n\u0026rsquo; code when the voice ends. The onset/offset do not need to be frame accurate. For cases when emotion code begins right out of missing: The face has not been visible and the first frame you can see the face again the infant is clearly displaying positive/negative emotion. Code the onset of the emotion code as the first frame when the face reappears. Use the “0” key to set the onset of the emotion event and simultaneously set the offset of \u0026lsquo;missing\u0026rsquo;. We want to preserve the 1ms difference between \u0026lsquo;missing\u0026rsquo; and the emotion code so we can know that that \u0026lsquo;missing\u0026rsquo; event was ended because of the onset of an emotion code.\nOffset Offset of a positive/negative emotion is the first frame the mom is clearly back to a neutral emotion through the face. The offset does not need to completely frame accurate, since emotion could end in any part of the face. The coder is looking to identify when any lay person would absolutely agree the mom is no longer showing any positive/negative emotion based on the face.\nIf the mom\u0026rsquo;s face returns to neutral for less than 5 frames during one emotion code (e.g. positive, then neutral for 4 frames, then back to positive), continue the \u0026lsquo;p\u0026rsquo; or \u0026lsquo;n\u0026rsquo; code. The coder would have to expend unneeded effort to identify and tag those offsets and onsets time, since reliability does not need to be frame accurate.\nFor cases when emotion code is ended by missing: The emotion event may or may not have ended but the coder can no longer see the face to code offset. Code the offset as the first frame the face completely is not visible (see \u0026lsquo;missing\u0026rsquo; code below). Use “0” key to set the offset of emotion and simultaneously code the onset of \u0026lsquo;missing\u0026rsquo;. We want to preserve the 1ms difference between the emotion code and \u0026lsquo;missing\u0026rsquo; so we can know that that \u0026lsquo;missing\u0026rsquo; event caused the offset of that emotion event.\n\u0026lt;.\u0026gt;\nCode \u0026lsquo;.\u0026rsquo; for missing when the mom\u0026rsquo;s face is completely not visible. The mom\u0026rsquo;s full face could turned away from the camera, mom\u0026rsquo;s head is completely off camera, or the mom is out of frame entirely. If the coder could see the emotion the mom is expressing in their face from a side or oblique angle, then do not code missing.\nIf the face is not visible but the mom is displaying clear negative emotion in the voice (e.g., mom yelling) then the missing code is ended and \u0026lsquo;n\u0026rsquo; is coded (see \u0026lsquo;n\u0026rsquo; code). Positive emotion cannot be coded by voice.\nOnset is the first frame in which the coder clearly cannot see the face. Offset is the first frame in which the coder can clearly see the face again. The onset and offset do not need to be completely frame accurate, since reliability does not need to be exact from accurate.\nIf the mom\u0026rsquo;s face was \u0026lsquo;missing\u0026rsquo; and then reappeared for less than 5 frames, don’t stop the \u0026lsquo;.\u0026rsquo; code to mark those few frames.\nHow to Code Set “JUMP-BACK-BY” key to 1 s.\nPlay with #8-PLAY in real time (1x speed) until the mom changes to clear positive or clear negative emotion or the face is clearly not visible. Focus on the mom\u0026rsquo;s face and do not be distracted by what the mom is saying or doing.\nPause with #5-STOP once you have identified a clear change in emotion or that the mom\u0026rsquo;s face is no longer visible at all. Shuttle back with #4-SHUTTLEBACK at 1\u0026frasl;8-1/4x speed to identify the onset. Use the mouth and eyes as the guide to onset. Press ENTER to set the onset as the frame where any lay person would say that mom is happy or sad. The coder may even feel happy or sad watching the mom\u0026rsquo;s face; use this as a guide for onset.\nThen hit #8-PLAY then #4-SHUTTLEBACK once to watch at 1/2x and look for the offset of that emotion or when the face comes completely back into view. For missing, if it seems like there may be a long stretch of missing (e.g. mom is on a different side of the room from the baby) then watch at 1x or 2x speed—but be listening in case there is negative emotion in the voice. Pause when you identify the offset.\nHit #1-JOGBACK and #3-JOGFORWARD to tag the frame the mom\u0026rsquo;s face is clearly not positive or not negative (returned to neutral) or the face is visible again to code emotion from.\nThen return to real time (1x speed) with #8-PLAY to watch for the next event.\n","date":1536465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546146000,"objectID":"3f6871144ae42f6085bf60ff678e408f","permalink":"/study/coding/emotion/","publishdate":"2018-09-09T00:00:00-04:00","relpermalink":"/study/coding/emotion/","section":"study","summary":"Coding emotion babyemotion \u0026lt;emotion_p-n\u0026gt;\nGeneral Orientation This code captures the times that the baby is clearly displaying positive or negative emotion through facial expressions. Times when the baby is in neutral emotion are not marked. Bouts of emotion as scored as events, where the grey spaces between emotion events mean the baby is neutral or emotion is unclear. Coders also mark times as “missing” when the baby\u0026rsquo;s face could not possibly be coded for emotion (e.","tags":null,"title":"","type":"docs"},{"authors":null,"categories":null,"content":" Coding gesture gesture \u0026lt;source_m-b\u0026gt;, \u0026lt;gesture_p-s-i-c\u0026gt;\nGeneral Orientation Gestures are segmented, durative, event-based behaviors. Watch the video paying attention to the communicative gestures used by the parent and the child. When coding for gesture, focus on the mother’s or baby’s hands and head.\nCode mother and baby gesture simultaneously in one pass. Then based on the \u0026lt;source\u0026gt; of the gesture, a script breaks apart mom and baby into separate babygesture and momgesture columns.\nOnly onsets are coded to expedite coding; offsets could be coded later if duration of gesture or overlap with specific other domains is of interest.\nValue List \u0026lt;source_m-b\u0026gt; m = mom b = baby h = mom holding baby\n\u0026lt;gesture_p-s-i-c\u0026gt; p = point s = show/hold up i = iconic gesture c = conventional gesture\nOperational Definitions \u0026lt;source_m-b\u0026gt; \u0026lt;m\u0026gt;: Code \u0026rsquo;m\u0026rsquo; if the mom is the source of the gesture. \u0026lt;b\u0026gt;: Code \u0026lsquo;b\u0026rsquo; if the baby is the source of the gesture.\n\u0026lt;gesture_p-s-i-c\u0026gt;\nGesturing by either mom or baby to the investigator, or anyone else in the room, should not be coded. The following should NOT be coded as gestures: tapping baby to get his/her attention; pushing an object away; hugging and kissing; one partner moving the other\u0026rsquo;s hand (e.g., to initiate contact, like proximity seeking); jerking the head to indicate “come here.”\n\u0026lt;p\u0026gt;\nCode \u0026lsquo;p\u0026rsquo; when the baby or mom extends their index finder to indicate reference to objects, people, events, or locations in the environment.\nOnset is the frame when the finger is fully extended in space toward a referent, or when the point finger is extended and makes contact with the object. Repetitive points should be coded as separate gesture events.\n\u0026lt;s\u0026gt;\nCode \u0026rsquo;s\u0026rsquo; when the baby or mom holds up an object to present it as if to say: “look at this” or “do you want this” or “I want you to take this”. Given that it’s not possible to distinguish intention, when a participant shows, offers, or gives an object (e.g., baby actually hands toy to mom, offering toy to mom but mom doesn’t take) code as \u0026rsquo;s\u0026rsquo;, to save decision-making time.\nOnset is the frame when the object is fully held up or out to show it. Repetitive instances of holding up or offering an object should be coded as separate gesture events.\n\u0026lt;i\u0026gt;\nCode \u0026lsquo;i\u0026rsquo; when the baby or mom engages in an iconic gesture. They are called iconic because they represent an object, idea, or action that can\u0026rsquo;t easily be referenced with a deictic (point/show) or conventional gesture. The movement of these gestures usually calls to mind something about the nature of the object, idea or action being referenced. For example, you could move your arms back and forth to represent running, or you could trace a square in the air with your finger, or flap your arms as if flying.\nOnset is the frame when the baby or mom has clearly begun the iconic gesture, and the coder can clearly identify this a gesture but does fall into the conventional gesture category (see \u0026lt;c\u0026gt;). Repetitive instances of an iconic gesture should be coded as separate gesture events.\n\u0026lt;c\u0026gt;\nCode \u0026lsquo;c\u0026rsquo; when the baby or mom engages in a conventional gesture. Conventional gestures are culturally-agreed-upon hand or head movements with a specific meaning, like nodding the head to mean “yes,” shaking the head to mean “no,” and moving the finger to lips to indicate “be quiet”.\nshaking head “no”    https://nyu.databrary.org/slot/14765/20618,23455/asset/76362/download?inline=true\nholding out hand for “give me”    https://nyu.databrary.org/slot/14765/20618,23455/asset/76362/download?inline=true\nIf a gesture is conventional, you should be able to understand its meaning just by seeing it in isolation, without knowing any of the context. Some additional examples of conventional gestures include: waving, clapping, flipping arms out to side to indicate “I don’t know’ or “where is it”, come here gesture (finger motions or palms), sit down gesture (pats ground), pickup gesture (child holds up arms to be picked up), thumbs up, shrugs, naughties (wag finger), hug me (hold arms out asking for hug), etc.\nOnset is the frame when the baby or mom has clearly begun the conventional gesture, and the coder can clearly identify this a gesture but does fall into the iconic gesture category (see \u0026lt;i\u0026gt;). Repetitive instances of a conventional gesture should be coded as separate gesture events.\nHow to Code Set “JUMP-BACK-BY” key to 2 s.\nGestures are best coded with the volume low or muted so that the language content does not confound the coding process.\nWatch in 1x speed until either mom or baby gestures. Focus on the mom’s and infant’s hands and head to identify instances of gestures.\nGestures are defined purely as they relate to the communicative nature of each action. The coder can establish whether something is communicative by looking at things like eye contact, conversational context, and the reaction of the person being spoken or gestured to. If the movement isn’t supposed to communicate anything, then it’s not a gesture. For example, a child might reach for an object and pick it up and look at it. This is an action, not a gesture. But, if the child points to the object to indicate its presence, or if the parent claps her hands to indicate “good job,” then these are gestures. (If there is significant ambiguity in whether a gesture is communicative, or how to code it, sound may be of assistance.)\nWhen the coder identifies a mom or baby gesturing, jump back 2 seconds and play the video again at ½ speed until the frame the gesture is clearly underway is found. Hit the = key (equal sign) to insert a point cell; so the current video frame becomes the onset and the offset.\nType \u0026rsquo;m\u0026rsquo; or \u0026lsquo;b\u0026rsquo; to indicate whether mom of the baby was the  of the gesture. Hit the TAB key to advance the cursor to , then type \u0026lsquo;p\u0026rsquo;, \u0026rsquo;s\u0026rsquo;, \u0026lsquo;i\u0026rsquo;, or \u0026lsquo;c\u0026rsquo; to indicate the type of gesture.\nSplitting Mom and Baby Gestures It\u0026rsquo;s faster to code mom and baby gesture together in one pass. But for consistency with the other coding passes, we want mom speech and baby gestures to be in two separate columns.\nRun the Split-MomBabyGesture.rb script to pull baby and mom gestures from the the gesture column into babygesture and momgesture columns.\nbabygesture \u0026lt;gesture_p-s-i-c\u0026gt;\nGeneral Orientation Contains gestures produced by the baby.\nThis column is automatically populated after the gesture pass is completed, using a Ruby script. All of the gestures tagged with \u0026lsquo;b\u0026rsquo; in  in the gesture column are transferred here. The onset and offset are equal, and set to the onset from the gesture column, which reflects the time when the coder was sure the gesture had begun.\nValue List \u0026lt;gesture_p-s-i-c\u0026gt;\np = point\ns = show/hold up\ni = iconic gesture\nc = conventional gesture\nmomgesture \u0026lt;gesture_p-s-i-c\u0026gt;\nGeneral Orientation Contains gestures produced by the mom.\nThis column is automatically populated after the gesture pass is completed, using a Ruby script. All of the gestures tagged with \u0026rsquo;m\u0026rsquo; in  in the gesture column are transferred here. The onset and offset are equal, and set to the onset from the gesture column, which reflects the time when the coder was sure the gesture had begun.\nValue List \u0026lt;gesture_p-s-i-c\u0026gt;\np = point\ns = show/hold up\ni = iconic gesture\nc = conventional gesture\n","date":1536465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546146000,"objectID":"8cf379fbeb4b8d5689e98874c62c63be","permalink":"/study/coding/gesture/","publishdate":"2018-09-09T00:00:00-04:00","relpermalink":"/study/coding/gesture/","section":"study","summary":"Coding gesture gesture \u0026lt;source_m-b\u0026gt;, \u0026lt;gesture_p-s-i-c\u0026gt;\nGeneral Orientation Gestures are segmented, durative, event-based behaviors. Watch the video paying attention to the communicative gestures used by the parent and the child. When coding for gesture, focus on the mother’s or baby’s hands and head.\nCode mother and baby gesture simultaneously in one pass. Then based on the \u0026lt;source\u0026gt; of the gesture, a script breaks apart mom and baby into separate babygesture and momgesture columns.","tags":null,"title":"","type":"docs"},{"authors":null,"categories":null,"content":" Coding locomotion babyloc \u0026lt;loc_l-f-h-c\u0026gt;\nGeneral Orientation This code captures the times that the baby is engaged in self-generated locomotion in any form (i.e., bum shuffling, scooting, belly crawling, hands-knees crawling, cruising, supported walking, independent walking, etc.).\n   https://nyu.databrary.org/slot/14765/-/asset/62749/download?inline=true\n   https://nyu.databrary.org/slot/14765/-/asset/62785/download?inline=true\nAlso included in this pass are the times when locomotion cannot occur because the baby is held,\n   https://nyu.databrary.org/slot/14765/-/asset/62751/download?inline=true\nand the times baby is constrained in baby furniture (e.g., a belted chair, highchair, or stroller).\n   https://nyu.databrary.org/slot/14765/-/asset/62753/download?inline=true\nCoders score only instances of baby-generated locomotion, and instances of falling, being held, or being constrained. Coders do not score instances where baby is stationary but could have locomoted. Bouts of locomotion are scored as events, where the gray spaces between cells mean the baby is stationary but not held and not constrained.\nCoders are watching/tagging the duration of each of these events (locomotion, falls, held, constrained) by marking onset/offset times. To determine locomotion, coders are watching for steps with the feet, the knees, or the bum. Any other movements that are not initiated from these three body locations are considered to be a transition between postures and are subsumed by stationary, because it is not locomotion.\nValue List l = locomotion\nf = fall\nm = mother constraining baby locomotion\nd = device constraining baby locomotion (high chair, stroller, carseat, etc.)\n. = when baby is off camera or the baby\u0026rsquo;s feet/knees/bum are off camera and coder cannot see or infer whether the baby is locomoting.\nOperational Definitions \u0026lt;l\u0026gt; Code “l” when the baby is engaged in self-generated locomotion in any form (i.e., bum shuffling, scooting, belly crawling, hands-knees crawling, climbing, cruising, supported walking, independent walking, etc.)\n   https://nyu.databrary.org/slot/14765/-/asset/62761/download?inline=true\n   https://nyu.databrary.org/slot/14765/-/asset/62769/download?inline=true\n   https://nyu.databrary.org/slot/14765/-/asset/62771/download?inline=true\n   https://nyu.databrary.org/slot/14765/-/asset/62773/download?inline=true\n   https://nyu.databrary.org/slot/14765/-/asset/62783/download?inline=true\nThis code counts locomotion regardless of whether the baby maintains balance independently or the baby\u0026rsquo;s balance is supported by a parent or external object/apparatus.\n   https://nyu.databrary.org/slot/14765/-/asset/62787/download?inline=true\nAny self-generated locomotion on a moving toy or baby furniture (e.g., a bicycle or bottomless car that the baby moves with their legs) counts as locomotion.\nLocomotion occurs when the entire body is displaced in any direction—forward, sideways, backward, in-place—space because the baby is taking a “step”.\n   https://nyu.databrary.org/slot/14765/-/asset/62775/download?inline=true\nA baby takes a “step” by shifting weight from one foot/knee onto the other (i.e., weight must be shifting onto a swinging foot in the air to count as moving; if not, this is stationary).\nOnset of a “step” is when the whole foot/knee comes up off the ground. A step can also happen if the foot doesn\u0026rsquo;t come off the ground, but the foot has to slide forward, backward or sideways. Marching in place, jumping, and hopping also count as locomotion. Offset is the frame when the baby takes the last step (with foot/knee) to pause in place (in the same posture such as walking to standing) or to transition to another stationary posture (e.g., upright walking to sitting). A pause must last at least 0.5 s.\nDo not include any movement with foot/knee as part of a transition to another posture (e.g., sit to upright/walk). The first walking/crawling step will be when the foot/knee moves forward in any direction. The final step in the bout has to be a real walking/crawling step (i.e., it is not the last half step or little attempt-step that looks like a transition into the sit). For example, if the baby transitions from sitting to crawling; the first step is after the transition ends and the last step is just before another transition begins.\nIf the feet/knees/bum are outside the camera view, code the locomotion bout if you can see the body moving and/or can infer that the baby is moving. If you are unsure as to whether the baby is moving or stationary (e.g., occlusion behind furniture or unclear video footage), then this bout should be coded as missing “.”, where the offset of the previous locomotion bout (just before the video occlusion or lack or view) should be set to the last frame where you can no longer see/infer the baby\u0026rsquo;s movement.\n\u0026lt;f\u0026gt;\nCode \u0026lsquo;f\u0026rsquo; if the baby loses control over his/her body (i.e., balance) and cannot recover on his/her own before his/her body hits the ground.\nAll falls count. They can happen while upright, on/off furniture or other elevation, while sitting, or while engaged in locomotion. Falls can happen while the mom is holding the baby\u0026rsquo;s hand or while the baby is holding onto furniture or another support.\n   https://nyu.databrary.org/slot/14765/-/asset/62747/download?inline=true\n   https://nyu.databrary.org/slot/14765/-/asset/63330/download?inline=true\n   https://nyu.databrary.org/slot/14765/-/asset/63336/download?inline=true\nOnset is frame when baby first begins to lose balance and Offset when baby\u0026rsquo;s body (as defined below) hits the floor.\nFrom an upright or squatting position: a loss of balance results in the hands, knees, or a toy in the hands hitting the ground; baby\u0026rsquo;s bum hitting the ground; or head hitting the ground.\nFrom a crawling position: a loss of balance results in the face, head, chest, or side of torso hitting the ground.\nFrom a sitting position: a loss of balance results in the head, chest, side of torso, or back hitting the ground.\nA loss of balance must occur before any of the body parts hit the ground. The baby must be out of his/her own control. Sometimes babies will actively let themselves lose control (e.g., plopping down into a sit, where they let themselves fall down into a sitting position). That is not a loss of balance but IS a loss of control and should count as a fall.\nIf the baby loses balance, but catches him/herself before the above body parts hit the ground, do not count as a fall.\nParent involved falls would only be coded as a fall if the parent catches the baby after the baby loses balance, effectively supporting the baby\u0026rsquo;s entire weight. In this scenario, the baby would have fallen if not for parent rescue (i.e., the body part would have hit the ground). Parents must catch after the baby has begun to lose balance. If the parent was already supporting the baby\u0026rsquo;s weight before a loss of balance, but baby\u0026rsquo;s body parts (e.g., hands, head, butt, etc.) do not touch the ground, then this is not a fall (it is supported walking).\n\u0026lt;m\u0026gt;\nCode \u0026rsquo;m\u0026rsquo; when the baby is being constrained and supported by the mother. The baby\u0026rsquo;s feet are not on the ground and is being held in the air by the mother. The mother\u0026rsquo;s arms are supplying support to the baby\u0026rsquo;s body by touching their torso. Do not count mother constraint when the baby is just sitting on the mother\u0026rsquo;s lap. During a mother constraint, the parent can be moving (carrying) or stationary (holding).\n   https://nyu.databrary.org/slot/14765/-/asset/62759/download?inline=true\n   https://nyu.databrary.org/slot/14765/-/asset/62797/download?inline=true\nOnset of hold is the last frame before the baby\u0026rsquo;s second foot (or bum if child was sitting, torso if child was laying down, second knee if crawling or knee walking) lifts up off the current surface and up into the air in mom\u0026rsquo;s arms.\nOffset of hold is when both feet touch the ground (or bum if mom places child in sitting, torso if mom places child laying down, both knees if mom places child in crawling or knee walking), and the baby starts supporting own weight in any posture.\nWhen mom is putting the baby back down, the onset for the immediate subsequent locomotion bout (if it happens) is when the second foot, hand, or knee touches the floor.\n\u0026lt;d\u0026gt;\nCode \u0026rsquo;d\u0026rsquo; if the baby is constrained in a device that restricts movement (e.g., a highchair, stroller, carseat, etc.). Device is not a couch, bed, or changing table unless there’s a strap, belt, or cord holding baby down. Device can never be household furniture not intended for children. Wooden or plastic child chair is not a device without straps.\nBaby walker is not a device. This would count as supported walking because that’s the whole point of a baby walker. Jolly jumper counts as a constrained device even though the baby is moving or jumping around.\nMom-constraint and device-constraint are likely to be continuous. Mom-constraint ends as she puts baby into device. Mom takes baby out of device is device-constraint transitioning into mom-constraint.\nOnset of constrained is when the baby\u0026rsquo;s butt first touches the restrictive device.\nOffset of constrained is when the butt leaves the device as the parent starts to take the baby out (usually by lifting them out).\nHow to Code Set “JUMP-BACK-BY” key to 1 s.\nEnable cell highlighting.\nWatch in real time for the baby\u0026rsquo;s movement.\nWatch baby\u0026rsquo;s feet and knees.\nAs soon as you see baby\u0026rsquo;s foot/knee lift up off of the ground; hit #5-STOP and then hit “JUMP-BACK-BY” to go back to the timestamp that is just before the lift. Then JOG forward by hitting #3-JOGFORWARD until you reach the Onset of that cell. If you go too far, you can JOG backward by hitting #1-JOGBACK. You will likely have to hit the JOG keys numerous times. If you feel that you have either jumped too far back or went too far forward, hold the JOG keys to move in either direction a bit faster. Hit ENTER to create a new cell at this Onset.\nNow, watch in real time to see when the baby stops moving. The Offset is when the baby stops moving for at least 0.5 s (the pause has to look and feel like an actual pause when you are watching in real time; don\u0026rsquo;t simply end a bout of locomotion because there was a 0.5-s pause, especially if it looks like the baby is about to take another step). The first frame when the foot/knee stops moving or when the foot settles into its final position (sometimes infants stop their walking bout on their tip-toes) is the offset. The same applies to sliding steps.\nTo set the Offset, use the same rules for mechanics as with the onset. Hit #5-STOP and then hit “JUMP-BACK-BY” to go back to the timestamp that is just before the lift. Then JOG forward by hitting #3-JOGFORWARD until you reach the Offset of that cell. If you go to far, you can JOG backward by hitting #1-JOGBACK. You will likely have to hit the JOG keys numerous times. If you feel that you have either jumped too far back or went too far forward, hold the JOG buttons to move in either direction a bit faster.\nmomloc \u0026lt;loc_l-f\u0026gt;\nGeneral Orientation This code captures the times that mom is engaged in locomotion or fell.\nBouts of locomotion are scored as events, where the gray spaces between cells mean the mom is stationary.\nCoders are watching/tagging each of these events by marking onset/offset times for the duration of locomotion bouts.\nCoders are watching for steps with the feet, the knees, or the bum.\n   https://nyu.databrary.org/slot/14765/-/asset/62765/download?inline=true\n   https://nyu.databrary.org/slot/14765/-/asset/62757/download?inline=true\nAny other movements that are not initiated from these three body locations is considered to be a transition between postures and is subsumed by stationary, as it is not locomotion.\nBouts that are coded as “.” means that mom is off camera or her legs are off camera, and coder cannot see or infer whether mom is stationary or moving.\nValue List l = locomotion\nf = fall\n. = when mother is off camera and coder cannot determine whether mom is moving or stationary\nOperational Definitions \u0026lt;l\u0026gt; Code \u0026lsquo;l\u0026rsquo; when the mom is engaged in locomotion of any form (i.e., walking, scooting, knee-walking, crawling).\n   https://nyu.databrary.org/slot/14765/-/asset/62779/download?inline=true\n   https://nyu.databrary.org/slot/14765/-/asset/62795/download?inline=true\n   https://nyu.databrary.org/slot/14765/-/asset/62797/download?inline=true\nIf you\u0026rsquo;re not sure whether the mom is moving or stationary (e.g., occlusion behind furniture or unclear video footage), then this is missing data and the offset of the locomotion bout should be set to the last frame where you can see mom.\nThe subsequent cell (where you cannot see anything or make an inference about movement) should be coded as “.” until you can see mom again.\nHowever, if you can infer that mom is moving or stationary (i.e., head is bobbing, shadow of the leg moving is visible, etc.) then include it in the same bout of locomotion, following the rules for pauses above.\n\u0026lt;f\u0026gt;\nCode \u0026lsquo;f\u0026rsquo; if the mom loses control over her body (i.e., balance) and cannot recover on her own before the body (bum, hands, torso) hits the ground.\n   https://nyu.databrary.org/slot/14765/-/asset/63360/download?inline=true\nAll falls count. They can happen while upright, on/off furniture or other elevation, while sitting, or while engaged in locomotion.\nFrom an upright or squatting position: a loss of balance results in the hands, knees, or a toy in the hands hitting the ground.\nHow to Code Set “JUMP-BACK-BY” key to 1 s.\nEnable cell highlighting on the controller.\nWatch in real time for the mom\u0026rsquo;s movement.\nWatch for the feet and knees.\nAs soon as you see mom\u0026rsquo;s foot/knee lift up off of the ground; hit #5-STOP and then hit “JUMP-BACK-BY” to go back to the timestamp that is just before the lift. Then JOG forward by hitting #3-JOGFORWARD until you reach the onset of that cell. If you go too far, you can JOG backward by hitting #1-JOGBACK. If you feel that you have either jumped too far back or went too far forward, hold the JOG keys to move in either direction a bit faster. Hit ENTER to create a new cell at this Onset.\nNow, watch in real time to see when the mom stops moving. The Offset is when the mom stops moving for at least 0.5 s (the pause has to look and feel like an actual pause when you are watching in real time; don\u0026rsquo;t simply end a bout of locomotion because there was a 0.5-s pause, especially if it looks like the mom is about to take another step). The first frame when the foot/knee stops moving or when the foot settles into its final position is the offset. The same applies to sliding steps.\nTo set the Offset, use the same rules for mechanics as with the Onset. Hit #5-STOP and then hit “JUMP-BACK-BY” to go back to the timestamp that is just before the lift. Then JOG forward by hitting #3-JOGFORWARD until you reach the Offset of that cell. If you go too far, you can JOG backward by hitting #1-JOGBACK. You will likely have to hit the JOG keys numerous times. If you feel that you have either jumped too far back or went too far forward, hold the JOG keys to move in either direction a bit faster.\n","date":1536465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546146000,"objectID":"430657b5f1416c505b37c62da074f4cf","permalink":"/study/coding/locomotion/","publishdate":"2018-09-09T00:00:00-04:00","relpermalink":"/study/coding/locomotion/","section":"study","summary":"Coding locomotion babyloc \u0026lt;loc_l-f-h-c\u0026gt;\nGeneral Orientation This code captures the times that the baby is engaged in self-generated locomotion in any form (i.e., bum shuffling, scooting, belly crawling, hands-knees crawling, cruising, supported walking, independent walking, etc.).\n   https://nyu.databrary.org/slot/14765/-/asset/62749/download?inline=true\n   https://nyu.databrary.org/slot/14765/-/asset/62785/download?inline=true\nAlso included in this pass are the times when locomotion cannot occur because the baby is held,\n   https://nyu.databrary.org/slot/14765/-/asset/62751/download?inline=true\nand the times baby is constrained in baby furniture (e.","tags":null,"title":"","type":"docs"},{"authors":null,"categories":null,"content":" Coding objects babyobject \u0026lt;obj\u0026gt;\nGeneral Orientation This code captures the times that the baby is manually engaged with an object.\n   https://nyu.databrary.org/slot/14765/-/asset/62793/download?inline=true\n   https://nyu.databrary.org/slot/14765/-/asset/62777/download?inline=true\nCoders score only when object events occur, not when they don\u0026rsquo;t occur. This is an event code, where gray spaces between cells mean that the baby is not engaged with an object.\nValue List o = object\n. = when baby is off camera and coder cannot determine whether baby has an object in hand.\nOperational Definitions \u0026lt;obj\u0026gt;\nObject = is defined as any manipulable, moveable item that may be detached and moved through space (e.g., toys, household items, and smaller moveable elements of larger objects like beads on busy box, doorknob).\n   https://nyu.databrary.org/slot/14765/-/asset/62763/download?inline=true\n   https://nyu.databrary.org/slot/14765/-/asset/63765/download?inline=true\nObjects may include large objects (i.e., a stroller, adult furniture, door, etc.) when baby moves them, thus, manually engaging with them. If the object never moves (e.g., the baby has a hand on the stroller but does not displace it), then this is not coded as \u0026lsquo;o.\u0026rsquo;\n   https://nyu.databrary.org/slot/14765/-/asset/63763/download?inline=true\nThe displacement rule is so that we can differentiate object engagement episodes from instances where baby is exploring a surface or resting hands on a surface for support.\n   https://nyu.databrary.org/slot/14765/-/asset/63767/download?inline=true\nThe infant does not have to be looking at the object for the event to count as an object engagement (e.g., baby is carrying object).\n   https://nyu.databrary.org/slot/14765/-/asset/62775/download?inline=true\nRiding on toys with wheels does not count as object, but this will be coded in babyloc pass.\nCode \u0026lsquo;o\u0026rsquo; if the baby is engaged with an object by making contact with the item with hand(s) and/or moving the item in space (e.g., carrying, pushing on the floor, etc.)\n   https://nyu.databrary.org/slot/14765/-/asset/62783/download?inline=true\nOnset is the frame when baby first causes the object to move while making contact with any part of the hand(s), not feet. Contact could be from any part of the hand (fingers, palm, side of hand). Movement could including lifting, holding, pressing, grasping, shaking, banging, or any other type of displacement event. DO NOT code onset, just when the hand touches the object if the object is not displaced (e.g., if they child touches a pillow but then 1 minute later actually grasps and moves it, code onset at the movement not when the hand touches the object). Offset is the frame when baby is no longer in manual contact with an object for at least 3 s. OR when the baby is in manual contact but the object is no longer being displaced (displacement includes holding, lifting) for at least 3 s. There is no minimum duration for baby to touch an object to be scored as \u0026lsquo;o,\u0026rsquo; but if infant is touching multiple objects, the offset of \u0026lsquo;o\u0026rsquo; object cell is when baby is no longer in manual contact with the last object contacted for 3+ s. If the baby is in manual contact with an object in one hand and makes contact with another object with their second hand, count this as the same bout.\n   https://nyu.databrary.org/slot/14765/-/asset/62755/download?inline=true\nHow to Code Set “JUMP-BACK-BY” key to 3 s.\nEnable cell highlighting.\nWatch in real time for the baby\u0026rsquo;s hand(s). As soon as you see the hand(s) touch an object (as defined above), continue watching for a couple of seconds to see if the baby moves/manipulates the object. Then, hit #4-SHUTTLEBACK to get to the onset of the cell. The Onset is the first frame when the baby makes manual contact with the item. Set this onset by hitting ENTER to set a new cell with that onset time. Now, continue watching the object bout in real time and set the Offset when the baby breaks manual contact or stops moving object (e.g., stroller) for at least 3 s. Once you\u0026rsquo;ve determined that the bout has ended, set the offset by hitting #5-STOP and then #4-SHUTTLEFORWARD or #6-SHUTTLEBACK to the last frame where the baby is no longer in manual contact with the item and/or when the baby is no longer moving it. Then, hit #9-SETOFFSET.\nContinue watching in real time for the next object bout. If the baby is holding an object while crawling or walking around, you can watch faster by SHUTTLING at 2x speed to find the end of the object engagement.\nTo check whether a 3-s pause has occurred between object engagements, go to the offset of the previous object cell and watch until you reach the next instance of \u0026lsquo;o\u0026rsquo;. Then, hit the \u0026lsquo;JUMP-BACK-BY\u0026rsquo; key and check to see if the previous cell lights up. If it does, then the two cells are \u0026lt;3 s apart and should be combined into one bout of \u0026lsquo;o\u0026rsquo;.\nmomobject \u0026lt;obj\u0026gt;\nGeneral Orientation This code captures the times that the mom is engaged with an object. Coders score only when object events occur, not when they don\u0026rsquo;t occur. This is an event code, where gray space in between cells means that the mom is not engaged with an object.\nValue List o = object.\n. = when mother is off camera and coder cannot determine whether she has an object in hand.\nOperational Definitions \u0026lt;obj\u0026gt;\nObject = is defined as any manipulable, moveable item that may be detached and moved through space (e.g., toys, household items). Object can include parts of a stationary object (e.g., doorknob on door, clasp on drawer) that can be moved or manipulated.\n   https://nyu.databrary.org/slot/14765/-/asset/62767/download?inline=true\n   https://nyu.databrary.org/slot/14765/-/asset/62781/download?inline=true\n   https://nyu.databrary.org/slot/14765/-/asset/62791/download?inline=true\n   https://nyu.databrary.org/slot/14765/-/asset/62781/download?inline=true\n   https://nyu.databrary.org/slot/14765/-/asset/62797/download?inline=true\nObject can include large objects that mom may move (chairs).\nCode \u0026lsquo;o\u0026rsquo; if mom is engaged with an object by making contact with the item with her hand(s). Onset is the frame when mom first makes contact with hands. Offset is the frame when mom is no longer in manual contact with an object for at least 3 s. If the mom has multiple items in hand, the Onset of object is when a hand(s) touched the first object in the multiple-object-bout and the Offset is when the hand(s) release the last object.\nIn cases of larger objects (i.e., a stroller, a box, a chair, a table, etc.), the object engagement begins when the object starts to move. If the large object never moves (e.g., the mom has a hand on the stroller but does not displace it), then this is not coded as \u0026lsquo;o\u0026rsquo;.\n   https://nyu.databrary.org/slot/14765/-/asset/63366/download?inline=true\nIf the mom is not in the camera view, code this with a \u0026lsquo;.\u0026rsquo; as missing data.\nHow to Code Set “JUMP-BACK-BY” key to 3 s.\nEnable cell highlighting.\nWatch in real-time for the mom\u0026rsquo;s hand(s). As soon as you see the hand(s) touch an object (as defined above), continue watching for a couple of seconds to see if the mom moves/manipulated the object (which would make this an instance of Object). Then, hit #4-SHUTTLEBACK to get to the onset of the cell. The Onset is the first frame when the mom makes manual contact with the item and moves it through space. Set this onset by hitting ENTER to set a new cell with that onset time. Now, continue watching the Object bout in real time and set the Offset when the mom breaks manual contact or stops moving the object for at least 3 s (i.e., Object bouts that are interrupted by gray space are more than 3 s apart.\nThere is no necessary minimum duration for object engagement during the \u0026lsquo;o\u0026rsquo; bout to be coded as Object. In other words, the mom can engage with an item or as little or as much time as they would like, however, the mom must make manual contact and move it through space to count.\nOnce you\u0026rsquo;ve determined that the bout has ended, set the offset by hitting #5-STOP and then #4-SHUTTLEFORWARD or #6-SHUTTLEBACK to the last frame where the mom if no longer in manual contact with the item and/or when the mom is no longer moving it. Then, hit #9-SETOFFSET.\nContinue watching in real time for the next object bout. If the mom is walking or crawling with an object, watch at 2x speed.\nDo not agonize. If the mom goes in and out of the camera view, but you know she is still holding the same object and has not put it down, code it in the same bout of \u0026lsquo;o\u0026rsquo;. Do not mark the “.” for every few seconds she is out of frame.\nCode as Object event if mom\u0026rsquo;s back is to the camera, but you see her arms moving and she overtly appears to be manipulating something—even if you can\u0026rsquo;t see exactly what it is.\nMany times, onsets and offsets are coded when mom goes in and out of frame. In these instances, hit the 0 key to set a continuous cell, whose onset is 1-ms after the previous cell.\n","date":1536465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546146000,"objectID":"12993933943b8f78b3ca1c24b908e03e","permalink":"/study/coding/objects/","publishdate":"2018-09-09T00:00:00-04:00","relpermalink":"/study/coding/objects/","section":"study","summary":"Coding objects babyobject \u0026lt;obj\u0026gt;\nGeneral Orientation This code captures the times that the baby is manually engaged with an object.\n   https://nyu.databrary.org/slot/14765/-/asset/62793/download?inline=true\n   https://nyu.databrary.org/slot/14765/-/asset/62777/download?inline=true\nCoders score only when object events occur, not when they don\u0026rsquo;t occur. This is an event code, where gray spaces between cells mean that the baby is not engaged with an object.\nValue List o = object\n. = when baby is off camera and coder cannot determine whether baby has an object in hand.","tags":null,"title":"","type":"docs"},{"authors":null,"categories":null,"content":" Transcribing transcribe \u0026lt;source_m-b\u0026gt;, \u0026lt;content\u0026gt;.\nGeneral Orientation The transcribe column is used to tag the onset of each utterance/vocalization by the mom and baby in a single pass. Then based on the \u0026lt;source\u0026gt; of the utterance/vocalization, the momspeech and babyvoc columns are automatically populated by a script.\nUtterance = a unit of speech separated by silence/pause, which can be a natural “period” as in end of a complete thought or sentence or a long pause (i.e., taking a breath). Utterances are coded as events (cells) separated by gray where no utterances are spoken. These are coded as events, where there is only one time that is tagged (onset), which is any time during the utterance. We do not code strict onsets and offset for the event; a single time during the utterance is the time coded.\nTranscribe all of mothers\u0026rsquo; utterances even if they are not baby-directed. But only transcribe communicative utterances; that is, there is no need to tag and transcribe non-speech, non-communicative sounds by the mother (e.g., making whistling noises, muttering to herself indistinguishably).\nParalinguistic utterances/vocalizations (e.g., laughing, crying, sighing, screaming) by the mom should be typed out (e.g., “ah”). Non-linguistic vocalizations by the baby are coded as “c” (a catch all for crying, laughing, screaming, grunting). Linguistic babbling, vowels, consonants, and combinations of the above by the baby that are not words are coded as “b”.\nValue List \u0026lt;source_m-b\u0026gt;\nm = mom\nb = baby\n\u0026lt;content\u0026gt;\nIf the content of the utterance can be heard clearly by the coder, then type transcript of each utterance in the cell.\nb = babbling, or vowel/consonant sound by the baby\nc = crying, screaming, grunting, laughing sound by the baby\nOperational Definitions \u0026lt;source_m-b\u0026gt;\n\u0026lt;m\u0026gt;: Code \u0026rsquo;m\u0026rsquo; if the mom is the source of the utterance. This code will be filled in automatically using quick keys.\n\u0026lt;b\u0026gt;: Code \u0026lsquo;b\u0026rsquo; if the baby is the source of the utterance. This code will be filled in automatically using quick keys.\n\u0026lt;content\u0026gt; transcribe utterance Type the complete utterance. Type everything in lower case, except for proper names (e.g., Mommy, I, Cheerios, Anna). Use apostrophes correctly for contractions and possessives (e.g., don\u0026rsquo;t, where\u0026rsquo;s, Daddy\u0026rsquo;s, Lily\u0026rsquo;s). Do not use “,” commas.\nTranscription: “snowmans dont drink coffee”  Your browser does not support the video tag.   https://nyu.databrary.org/slot/14765/-/asset/63400/download?inline=true \nTranscription: “un caballo”  Your browser does not support the video tag.   https://nyu.databrary.org/slot/14765/-/asset/63406/download?inline=true \nTranscription: “momma”  Your browser does not support the video tag.   https://nyu.databrary.org/slot/14765/-/asset/63404/download?inline=true \nTranscription: “woof woof”  Your browser does not support the video tag.   https://nyu.databrary.org/slot/14765/-/asset/63414/download?inline=true \nPut a question mark “?” at the end of any utterance that is a question.\nTranscription: “want cheerios?”  Your browser does not support the video tag.   https://nyu.databrary.org/slot/14765/-/asset/63408/download?inline=true \nIndividual letters (e.g., mom spells out zoo as “z” “o” “o”) need to be marked with an @ (at symbol) so that they\u0026rsquo;re not confused with actual words, for example z@ o@ o@. Use existing rules for utterances to decide if each letter is it\u0026rsquo;s own utterance.\nAny utterance that is unintelligible or hard to decipher, code as “xxx”. This could be the full utterance: for example, the mom says multiple words but they are all unintelligible, so the entire code is “xxx”. Or part of the utterance is intelligible, but part is not: for example, the mom says “give me” and what she says to give is unintelligible, so code “give me xxx”.\nTranscription: “xxx”  Your browser does not support the video tag.   https://nyu.databrary.org/slot/14765/-/asset/63412/download?inline=true\nIn case of language-functioning sounds by the mom or baby, these are typed out as words phonetically as specified below: Ahem (ready to speak), Ay (surprise), Huhuh (no), Hmm (thinking or questioning), Mmhm (yes), Sh (shush), Uhhuh (yes), Uhoh (blunder), uhuh (no), Yeah (yes), Whee (excitement), Whoah (surprise), Whoops (mistake)\nIf you encounter a mouth sound that the mom makes, which is communicative but cannot be transcribed phonetically (e.g., lip sucking/kissing sound to call the baby over), write out the sound of the vocalization in brackets (e.g. [lip sucking/kissing]).\nTranscription: “[lip sucking/kissing]”  Your browser does not support the video tag.   https://nyu.databrary.org/slot/14765/-/asset/63398/download?inline=true\nOnly write out communicative sounds: for instance, if the mom whistles to get the baby\u0026rsquo;s attention write out [whistles], but if the mom is just whistling to herself as she cooks then do not tag as an utterance.\n\u0026lt;b\u0026gt; Code \u0026lsquo;b\u0026rsquo; if the baby is not saying a full language phrase or sentence. Could be babbling, by saying one or more consonant-vowel pairs. Or could be just a vowel.\n Your browser does not support the video tag.   https://nyu.databrary.org/slot/14765/-/asset/63338/download?inline=true\n Your browser does not support the video tag.   https://nyu.databrary.org/slot/14765/-/asset/63340/download?inline=true\n Your browser does not support the video tag.   https://nyu.databrary.org/slot/14765/-/asset/63342/download?inline=true\n Your browser does not support the video tag.   https://nyu.databrary.org/slot/14765/-/asset/63344/download?inline=true\nIf you are unsure if it was a language phrase that you can transcribe or was just a babble, then mark as unintelligible “xxx” and make a comment. Either relisten or come back after getting more context later in the video.\n Your browser does not support the video tag.   https://nyu.databrary.org/slot/14765/-/asset/63410/download?inline=true\n\u0026lt;c\u0026gt; Code \u0026lsquo;c\u0026rsquo; if the baby is making a non-language-like vocalization. For instance, crying, screaming, laughing, grunting. Any vocalization that is not a consonant-vowel babble or vowel alone. These should be easy to distinguish from babbling, because they serve a different communicative function.\n Your browser does not support the video tag.   https://nyu.databrary.org/slot/14765/-/asset/63746/download?inline=true\n Your browser does not support the video tag.   https://nyu.databrary.org/slot/14765/-/asset/63747/download?inline=true\n Your browser does not support the video tag.   https://nyu.databrary.org/slot/14765/-/asset/63753/download?inline=true\nHow to Transcribe Set “Jump back by” on the Controller to 2 seconds.\nTranscribing is done in two iterative passes through a small section of the video (roughly 1-2 minutes). The first part is tagging utterances for about 1-2 minutes (until a good break in activity is reached) and the second part is looping back over that same portion of the video to transcribe utterances.\nTagging Utterances Turn on Quick Keys mode by hitting Shift-Cmnd-K (or selecting Spreadsheet\u0026gt;Enable Quick Keys from the menu bar). You will see \u0026lt;QUICK KEY MODE\u0026gt; in the spreadsheet window header. This will enable a function that every time an alphanumeric key is pressed, a new point cell (onset=offset) is inserted at the current time in the data controller, and the alphanumeric key pressed will be inserted as the code of the first argument.\nPlace your left index finger on the “m” key and your left ring finger on the “b” key. The right fingers should be on pause and shuttle forward and back. Play the video at 1\u0026frasl;2 speed (or 1\u0026frasl;4 speed if a fast exchange of utterances is happening). Press the “m” or “b” key every time the mom or baby has an utterance, while you play the video back. Insert cells as soon as you hear something. Be as alert and attentive as possible.\nIf you think you hear an utterance, tag it. It\u0026rsquo;s much better to be fast and insert extra cells, rather than judge yourself and have to go back later to fix the time or insert a cell for an utterance you missed. You can easily delete cells using shortcut keys. You can also fix the \u0026lt;source\u0026gt; code later if you hit the wrong key. Note, offsets are not coded. Onsets are as close to the utterance onset as you possibly can get. So optimize your attention and coding for speed of tagging.\nThe best strategy is to have an unbroken playback session of 1-2 mins where you are just tagging utterances without stopping. Stop playback once you\u0026rsquo;ve tagged 30-40 cells, 1-2 mins have elapsed, or you hit a good breaking point in an activity (e.g. baby moves onto playing with a new toy). Try to stop a tagging utterances past as soon as you tag a new utterance, rather than playing further into silence of the video; that way you can jump to and pick up right where you left off at an utterance for the next tagging pass, instead of potentially re-playing the same part of the video.\nNow, turn off Quick Keys (Shift-Cmnd-K). Run the addtime_transcribe.rb script. This will add 500 ms to the offset, which will help in highlighting in the next step.\nTranscribe Utterances Turn on Highlight and Focus Mode by hitting Shift-Cmnd-F (or selecting Spreadsheet\u0026gt;Enable Highlight and Focus Mode from the menu bar). This will highlight each cell as you loop back through the 1-2 mins you just tagged utterances in and put the focus of data entry (cursor) into the first uncoded argument in that cell (which will be \u0026lt;content\u0026gt;).\nSCROLL or ARROW up to the first cell from the most recent utterance tagging done. Jump to that first cell (+ key) and then JUMP-BACK-BY 2 s (- key). Playback the video at 1x speed. Listen to each utterance within the context of the ongoing stream of speech. JUMP-BACK and re-listen as many times as needed until you are sure of the utterance. Once you are sure of the utterance, stop playback and transcribe the utterance or insert the appropriate code (for the baby).\nIf you go past an utterance and missed transcribing it, hit the jump back key until you are right before it. If you get lost in the transcription, JUMP BACK 2-3 cells (by arrowing or jump back key) before where you lost track of transcribing. It\u0026rsquo;s much better to use the keyboard to navigate and loop back (jump back or arrow up or down) rather than mousing. (Note: you may need to mouse into the argument of the first cell in a section after tagging utterances). If you mouse and jump around, you will get lost; stay in “looping” mode throughout transcription even if it means listening multiple times to a single section.\nIf you find a cell for an utterance that was tagged by mistake (you thought there was an utterance but there wasn\u0026rsquo;t) then delete that cell. JUMP-BACK-BY 2 s before the cell you deleted and confirm there was no utterance and that the next utterance is tagged at the correct time. (Note: you may need to mouse into a cell after deleting).\nIf you need to change the onset of an utterance, ARROW into it (or let auto focus move you into it) and hit the 7 key to set onset to the current time. Do not worry about setting or fixing the offset.\nIf you missed tagging an utterance in the first part, find the time of the utterance onset while you are transcribing. Hit ENTER and set the offset (same time as onset) using the 9 key. Code “m” or “b” for \u0026lt;source\u0026gt;, then tab into \u0026lt;content\u0026gt; and transcribe.\nTurn off Highlight and Focus Mode. Save the file. Now turn back on Quick Keys, jump to the onset of the last cell transcribed, and revert back to the coding strategy for tagging utterances.\nSplitting Mom and Baby Utterances It\u0026rsquo;s easier and faster to tag and transcribe mom and baby together in one pass. But for later coding, we want mom speech and baby vocalizations to be in two separate columns.\nRun the splitmombabytranscribe.rb script to pull mom and baby utterances into their appropriate columns.\nAt this step, you can also run the createmombabyutterancetype.rb script to insert the momutterancetype and babyutterancetype columns and insert cells for every tagged utterance.\n","date":1536465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546146000,"objectID":"05c2de406ba2aa44cd2082f0c6a71d82","permalink":"/study/coding/transcription/","publishdate":"2018-09-09T00:00:00-04:00","relpermalink":"/study/coding/transcription/","section":"study","summary":"Transcribing transcribe \u0026lt;source_m-b\u0026gt;, \u0026lt;content\u0026gt;.\nGeneral Orientation The transcribe column is used to tag the onset of each utterance/vocalization by the mom and baby in a single pass. Then based on the \u0026lt;source\u0026gt; of the utterance/vocalization, the momspeech and babyvoc columns are automatically populated by a script.\nUtterance = a unit of speech separated by silence/pause, which can be a natural “period” as in end of a complete thought or sentence or a long pause (i.","tags":null,"title":"","type":"docs"},{"authors":null,"categories":null,"content":" Collaborating Investigators The following provides information about which researchers are collecting data and which labs are coding and providing overall guidance on the project.\n   Last First Institution Collection_role     Adolph Karen New York University Collecting   Amso Dima Brown University Not collecting   Barr Rachel Georgetown University Collecting   Berenbaum Sheri Penn State University Not collecting   Bornstein Marc SRCD Not collecting   Boudreau Jean-Paul Mount Alison University Not collecting   Bradley Bob Arizona State University Not collecting   Brandone Amanda Lehigh University Not collecting   Brooker Rebecca Texas A\u0026amp;M Not collecting   Buss Kristin Penn State University Not collecting   Casasola Marianella Cornell University Collecting   Chi Guangqing Penn State University Not collecting   Claxton Laura Purdue University Collecting   Davis Elizabeth University of California at Riverside Not collecting   de Barbaro Kaya University of Texas at Austin Not collecting   Dusing Stacey Virginia Commonwealth University Collecting   Evans Gary Cornell University Not collecting   Fausey Caitlin University of Oregon Collecting   Franchack John University of California at Riverside Collecting   Frank Mike Stanford University Collecting   Frick Janet University of Georgia Collecting   Gill Simone Boston University Collecting   Gilmore Rick Penn State University Collecting   Goldin-Meadow Susan University of Chicago Not collecting   Goldstein Mike Cornell University Not collecting   Haddad Jeff Purdue University Not collecting   Halim May Ling California State University at Long Beach Collecting   Hane Amie Williams College Not collecting   Hauck Janet Michigan State University Not collecting   Heathcock Jill Ohio State University Collecting   Henderson Heather University of Waterloo Not collecting   Hirsh-Pasek Kathy Temple University Not collecting   Iverson Jana University of Pittsburgh Collecting   Karasik Lana CUNY \u0026ndash; College of Staten Island Collecting   Lee Do Kyeong California State University at Fullerton Collecting   Lee Mei-Hua Michigan State University Collecting   Legare Cristine University of Texas at Austin Collecting   Lew-Williams Casey Princeton University Collecting   Libertus Klaus University of Pittsburgh Not collecting   LoBue Vanessa Rutgers University Collecting   Lockman Jeff Tulane University Collecting   MacWhinney Brian Carnegie Mellon University Not collecting   Messinger Dan University of Miami Collecting   Naigles Letitia University of Connecticut Not collecting   Namy Laura Society for Research In Child Development Not collecting   Needham Amy Vanderbilt University Collecting   Newcombe Nora Temple University Not collecting   Oakes Lisa University of California at Davis Collecting   Olson Kristina University of Washington Not collecting   Perez-Edgar Koraly Penn State University Not collecting   Pomerantz Eva University of Illinois at Urbana-Champagne Not collecting   Prosser Laura Children\u0026rsquo;s Hospital of Philadelphia Collecting   Rowe Meredith Harvard University Not collecting   Schmuckler Mark University of Toronto Scarborough Not collecting   Sheya Adam University of Connecticut Not collecting   Soderstrom Melanie University of Manitoba Not collecting   Song Lulu Brooklyn College Not collecting   Tamis-LeMonda Catherine New York University Not collecting   Vishton Peter College of William \u0026amp; Mary Collecting   Walle Eric University of California at Merced Not collecting   Wang Su-hua University of California at Santa Cruz Collecting   Warlaumont Anne University of California at Los Angeles Collecting   Yoshida Hanako University of Houston Collecting   Yu Chen Indiana University Collecting   Yurovsky Dan University of Chicago Collecting    ","date":1536465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546146000,"objectID":"53a458bc0be97fa75a3a9393b86b37bb","permalink":"/study/people/collaborators/","publishdate":"2018-09-09T00:00:00-04:00","relpermalink":"/study/people/collaborators/","section":"study","summary":" Collaborating Investigators The following provides information about which researchers are collecting data and which labs are coding and providing overall guidance on the project.\n   Last First Institution Collection_role     Adolph Karen New York University Collecting   Amso Dima Brown University Not collecting   Barr Rachel Georgetown University Collecting   Berenbaum Sheri Penn State University Not collecting   Bornstein Marc SRCD Not collecting   Boudreau Jean-Paul Mount Alison University Not collecting   Bradley Bob Arizona State University Not collecting   Brandone Amanda Lehigh University Not collecting   Brooker Rebecca Texas A\u0026amp;M Not collecting   Buss Kristin Penn State University Not collecting   Casasola Marianella Cornell University Collecting   Chi Guangqing Penn State University Not collecting   Claxton Laura Purdue University Collecting   Davis Elizabeth University of California at Riverside Not collecting   de Barbaro Kaya University of Texas at Austin Not collecting   Dusing Stacey Virginia Commonwealth University Collecting   Evans Gary Cornell University Not collecting   Fausey Caitlin University of Oregon Collecting   Franchack John University of California at Riverside Collecting   Frank Mike Stanford University Collecting   Frick Janet University of Georgia Collecting   Gill Simone Boston University Collecting   Gilmore Rick Penn State University Collecting   Goldin-Meadow Susan University of Chicago Not collecting   Goldstein Mike Cornell University Not collecting   Haddad Jeff Purdue University Not collecting   Halim May Ling California State University at Long Beach Collecting   Hane Amie Williams College Not collecting   Hauck Janet Michigan State University Not collecting   Heathcock Jill Ohio State University Collecting   Henderson Heather University of Waterloo Not collecting   Hirsh-Pasek Kathy Temple University Not collecting   Iverson Jana University of Pittsburgh Collecting   Karasik Lana CUNY \u0026ndash; College of Staten Island Collecting   Lee Do Kyeong California State University at Fullerton Collecting   Lee Mei-Hua Michigan State University Collecting   Legare Cristine University of Texas at Austin Collecting   Lew-Williams Casey Princeton University Collecting   Libertus Klaus University of Pittsburgh Not collecting   LoBue Vanessa Rutgers University Collecting   Lockman Jeff Tulane University Collecting   MacWhinney Brian Carnegie Mellon University Not collecting   Messinger Dan University of Miami Collecting   Naigles Letitia University of Connecticut Not collecting   Namy Laura Society for Research In Child Development Not collecting   Needham Amy Vanderbilt University Collecting   Newcombe Nora Temple University Not collecting   Oakes Lisa University of California at Davis Collecting   Olson Kristina University of Washington Not collecting   Perez-Edgar Koraly Penn State University Not collecting   Pomerantz Eva University of Illinois at Urbana-Champagne Not collecting   Prosser Laura Children\u0026rsquo;s Hospital of Philadelphia Collecting   Rowe Meredith Harvard University Not collecting   Schmuckler Mark University of Toronto Scarborough Not collecting   Sheya Adam University of Connecticut Not collecting   Soderstrom Melanie University of Manitoba Not collecting   Song Lulu Brooklyn College Not collecting   Tamis-LeMonda Catherine New York University Not collecting   Vishton Peter College of William \u0026amp; Mary Collecting   Walle Eric University of California at Merced Not collecting   Wang Su-hua University of California at Santa Cruz Collecting   Warlaumont Anne University of California at Los Angeles Collecting   Yoshida Hanako University of Houston Collecting   Yu Chen Indiana University Collecting   Yurovsky Dan University of Chicago Collecting    ","tags":null,"title":"","type":"docs"},{"authors":null,"categories":null,"content":"","date":1536465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546146000,"objectID":"a1740dd223686a9a7dc9518bd09d440f","permalink":"/study/coding/coding_manual/","publishdate":"2018-09-09T00:00:00-04:00","relpermalink":"/study/coding/coding_manual/","section":"study","summary":"","tags":null,"title":"Coding Manual","type":"docs"},{"authors":["Karen Adolph","Catherine Tamis-LeMonda","Rick Gilmore","Kasey Soska"],"categories":null,"content":"","date":1530244800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530244800,"objectID":"c266761647aae56e0678cebedbff265a","permalink":"/talk/play-summit-2018/","publishdate":"2018-06-29T00:00:00-04:00","relpermalink":"/talk/play-summit-2018/","section":"talk","summary":"The PLAY team met in Philadelphia to finalize details about the project protocol.","tags":["meeting","protocol","coding"],"title":"PLAY Summit @ ICIS 2018 Philadelphia","type":"talk"},{"authors":["Karen Adolph","Catherine Tamis-LeMonda","Rick Gilmore"],"categories":null,"content":"We are grateful to the Eunice Kennedy Shriver National Institute of Child Health and Human Development (NICHD), New York University (NYU), the Office of Behavioral and Social Sciences Research (OBSSR), the Society for Research in Child Development (SRCD), and the LEGO Foundation (LEGO) for providing support.\n","date":1481864400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1481864400,"objectID":"0518214f31f7fc7871af804269882f10","permalink":"/talk/nichd-workshop-2016/","publishdate":"2016-12-16T00:00:00-05:00","relpermalink":"/talk/nichd-workshop-2016/","section":"talk","summary":"Researchers met at NICHD headquarters to discuss how to study natural behavior in the home.","tags":["meeting","protocol","coding","video"],"title":"NICHD workshop 2016 Bethesda","type":"talk"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461729600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461729600,"objectID":"e37b6f6adba54b70de46abfc8cddb434","permalink":"/measures/demographics/","publishdate":"2016-04-27T00:00:00-04:00","relpermalink":"/measures/demographics/","section":"measures","summary":"Family structure and characteristics.","tags":["demographics","questionnaires"],"title":"Demographics","type":"measures"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461729600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461729600,"objectID":"30a1b38be3a581c95842f9edb031f578","permalink":"/measures/emotion/","publishdate":"2016-04-27T00:00:00-04:00","relpermalink":"/measures/emotion/","section":"measures","summary":"Emotional expression \u0026 temperament","tags":["emotion","questionnaires","video"],"title":"Emotion","type":"measures"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461729600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461729600,"objectID":"eab5032b3d2ff9c375027061df24a677","permalink":"/measures/environment/","publishdate":"2016-04-27T00:00:00-04:00","relpermalink":"/measures/environment/","section":"measures","summary":"Home and neighborhood environment.","tags":["environment","questionnaires","neighborhood"],"title":"Environment","type":"measures"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461729600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461729600,"objectID":"dbfb0dadcda6973148bdf5de7b2e938d","permalink":"/measures/health/","publishdate":"2016-04-27T00:00:00-04:00","relpermalink":"/measures/health/","section":"measures","summary":"Child and family health.","tags":["health","questionnaires"],"title":"Health","type":"measures"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461729600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461729600,"objectID":"68fef8eafde93137c1c13e9b24676f62","permalink":"/measures/language/","publishdate":"2016-04-27T00:00:00-04:00","relpermalink":"/measures/language/","section":"measures","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus.","tags":["language","questionnaires","gesture","transcription"],"title":"Language \u0026 Communication","type":"measures"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461729600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461729600,"objectID":"d2bed72cb346a3376c579079a922ba01","permalink":"/measures/physical_activity/","publishdate":"2016-04-27T00:00:00-04:00","relpermalink":"/measures/physical_activity/","section":"measures","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus.","tags":["locomotion","questionnaires","physical activity","video"],"title":"Locomotion \u0026 Physical Activity","type":"measures"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461729600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461729600,"objectID":"91089b8137575635f04030ae6a9f4137","permalink":"/measures/media_use/","publishdate":"2016-04-27T00:00:00-04:00","relpermalink":"/measures/media_use/","section":"measures","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus.","tags":["locomotion","questionnaires","physical activity","video"],"title":"Media use","type":"measures"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461729600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461729600,"objectID":"aaf8b43cdf0eae2da5df67389da5418a","permalink":"/measures/object_interaction/","publishdate":"2016-04-27T00:00:00-04:00","relpermalink":"/measures/object_interaction/","section":"measures","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus.","tags":["objects","questionnaires","video"],"title":"Objects","type":"measures"},{"authors":null,"categories":null,"content":" 1 hour of natural play Mothers and infants will be recorded for an hour while they go about their daily activities.\nDyadic play Mothers and infants will be recorded for 3 minutes while they play together with a toy.\nSolitary play Infants will be recorded for 3 minutes while they play with a toy.\nHome tour The experimenter will record a tour of the home narrated by the mother.\n","date":1461729600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461729600,"objectID":"5c86d07734877080998978a427fe2135","permalink":"/measures/video/","publishdate":"2016-04-27T00:00:00-04:00","relpermalink":"/measures/video/","section":"measures","summary":"1 hour of natural play Mothers and infants will be recorded for an hour while they go about their daily activities.\nDyadic play Mothers and infants will be recorded for 3 minutes while they play together with a toy.\nSolitary play Infants will be recorded for 3 minutes while they play with a toy.\nHome tour The experimenter will record a tour of the home narrated by the mother.","tags":["video"],"title":"Video","type":"measures"},{"authors":["GA Cushen"],"categories":null,"content":"More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n","date":1441080000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441080000,"objectID":"d77fa4a74076ffcd7ca6c21cfc27a4b2","permalink":"/publication/person-re-id/","publishdate":"2015-09-01T00:00:00-04:00","relpermalink":"/publication/person-re-id/","section":"publication","summary":"Person re-identification is a critical security task for recognizing a person across spatially disjoint sensors. Previous work can be computationally intensive and is mainly based on low-level cues extracted from RGB data and implemented on a PC for a fixed sensor network (such as traditional CCTV). We present a practical and efficient framework for mobile devices (such as smart phones and robots) where high-level semantic soft biometrics are extracted from RGB and depth data. By combining these cues, our approach attempts to provide robustness to noise, illumination, and minor variations in clothing. This mobile approach may be particularly useful for the identification of persons in areas ill-served by fixed sensors or for tasks where the sensor position and direction need to dynamically adapt to a target. Results on the BIWI dataset are preliminary but encouraging. Further evaluation and demonstration of the system will be available on our website.","tags":[],"title":"A Person Re-Identification System For Mobile Devices","type":"publication"},{"authors":["GA Cushen","MS Nixon"],"categories":null,"content":"More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n","date":1372651200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372651200,"objectID":"2b4d919e3cf73dfcd0063c88fe01cb00","permalink":"/publication/clothing-search/","publishdate":"2013-07-01T00:00:00-04:00","relpermalink":"/publication/clothing-search/","section":"publication","summary":"We present a mobile visual clothing search system whereby a smart phone user can either choose a social networking photo or take a new photo of a person wearing clothing of interest and search for similar clothing in a retail database. From the query image, the person is detected, clothing is segmented, and clothing features are extracted and quantized. The information is sent from the phone client to a server, where the feature vector of the query image is used to retrieve similar clothing products from online databases. The phone's GPS location is used to re-rank results by retail store location. State of the art work focuses primarily on the recognition of a diverse range of clothing offline and pays little attention to practical applications. Evaluated on a challenging dataset, the system is relatively fast and achieves promising results.","tags":[],"title":"Mobile visual clothing search","type":"publication"},{"authors":null,"categories":null,"content":" +++ title = “”\ndate = 2018-12-30T00:00:00 lastmod = 2019-01-07T00:00:00\ndraft = false # Is this a draft? true/false toc = true # Show table of contents? true/false type = “docs” # Do not modify.\nAdd menu entry to sidebar. [menu.protocol] name = “Visit Prep” weight = 3 +++\n Preparing for Visit Prepare paperwork Write Participant ID on all paperwork (consents and questionnaires). Fill out locomotor milestone worksheet.\n Pack equipment  Camera, SD card and extra battery\n Mic Laser Measure Solitary play toy Dyadic play toy Yoga mat Tablet with app for questionnaires (if mom speaks English and/or Spanish, bring both versions of MacArthur), study consent form, Databrary sharing release form, and decibel meter. Answer choice sheet with response scales Participant payment Paper copies of all questionnaires, MCDI, and consent and Databrary forms in case of tablet failure Tools for body dimensions (Height and Weight)- TBD    ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"34a97e18ace0ab2bba06d4ae53d36f86","permalink":"/study/protocol/visit_prep.rmarkdown/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/study/protocol/visit_prep.rmarkdown/","section":"study","summary":"+++ title = “”\ndate = 2018-12-30T00:00:00 lastmod = 2019-01-07T00:00:00\ndraft = false # Is this a draft? true/false toc = true # Show table of contents? true/false type = “docs” # Do not modify.\nAdd menu entry to sidebar. [menu.protocol] name = “Visit Prep” weight = 3 +++\n Preparing for Visit Prepare paperwork Write Participant ID on all paperwork (consents and questionnaires). Fill out locomotor milestone worksheet.","tags":null,"title":"","type":"study"},{"authors":null,"categories":null,"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c2915ec5da95791851caafdcba9664af","permalink":"/slides/example-slides/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/example-slides/","section":"slides","summary":"Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$","tags":null,"title":"Slides","type":"slides"}]